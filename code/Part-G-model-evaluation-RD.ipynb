{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068701ec",
   "metadata": {},
   "source": [
    "## Part C: Compute evaluation performances \n",
    "\n",
    "This notebook contains all the code used to compute the evaluation performances (space and space-time validation) of the models calibrated using the three geology maps to infer their respective HRUs. \n",
    "\n",
    "\n",
    "Alongside the other notebooks, it covers all the analysis performed in: \"How landscape data quality affects our perception of dominant processes in large-sample hydrology studies?\" paper by do Nascimento et al. (in review). To be able to run this notebook, please ensure that you have downloaded the acompanying data of the paper. All links can be found in the data section of the paper.\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbd754",
   "metadata": {},
   "source": [
    "# Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5becc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spotpy\n",
    "import time\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import hydroanalysis\n",
    "from utils.functions import find_max_unique_rows\n",
    "from utils.functions import find_iterative_immediate_downstream\n",
    "import geopandas as gpd\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276cde8c",
   "metadata": {},
   "source": [
    "## Set the path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0efd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows\n",
    "path_estreams = r'C:\\Users\\nascimth\\Documents\\data\\EStreams'\n",
    "\n",
    "## Mac\n",
    "#path_estreams = r'/Users/thiagomedeirosdonascimento/Downloads/Python 2/Scripts/estreams_part_b/data/EStreams'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfe0e2",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ca750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset network\n",
    "network_estreams = pd.read_csv(path_estreams+'/streamflow_gauges/estreams_gauging_stations.csv', encoding='utf-8')\n",
    "network_estreams.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "network_estreams['start_date'] = pd.to_datetime(network_estreams['start_date'])\n",
    "network_estreams['end_date'] = pd.to_datetime(network_estreams['end_date'])\n",
    "\n",
    "# Convert to list both the nested_catchments and the duplicated_suspect columns\n",
    "network_estreams['nested_catchments'] = network_estreams['nested_catchments'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Remove the brackets and handle NaN values\n",
    "network_estreams['duplicated_suspect'] = network_estreams['duplicated_suspect'].apply(\n",
    "    lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x)\n",
    "\n",
    "# Set the nested catchments as a dataframe\n",
    "nested_catchments = pd.DataFrame(network_estreams['nested_catchments'])\n",
    "\n",
    "# Now we add the outlet to the list (IF it was not before):\n",
    "# Ensure that the basin_id is in the nested_catchments\n",
    "for basin_id in nested_catchments.index:\n",
    "    if basin_id not in nested_catchments.at[basin_id, 'nested_catchments']:\n",
    "        nested_catchments.at[basin_id, 'nested_catchments'].append(basin_id)\n",
    "\n",
    "\n",
    "\n",
    "# Attributes already filtered previously:\n",
    "#estreams_attributes = pd.read_csv('data/exploration/estreams_attributes_filtered_moselle_sm_su_tog.csv', encoding='utf-8')\n",
    "estreams_attributes = pd.read_csv('../data/estreams_attributes_filtered_quality_geology_v01.csv', encoding='utf-8')\n",
    "\n",
    "estreams_attributes.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Convert to list both the nested_catchments and the duplicated_suspect columns\n",
    "estreams_attributes['nested_catchments'] = estreams_attributes['nested_catchments'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Remove the brackets and handle NaN values\n",
    "estreams_attributes['duplicated_suspect'] = estreams_attributes['duplicated_suspect'].apply(\n",
    "    lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x)\n",
    "\n",
    "estreams_attributes.sort_index(inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d63e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geological attributes (regional scale)\n",
    "geology_regional_31_classes_moselle = pd.read_csv(\"../data/estreams_geology_moselle_regional_attributes.csv\", encoding='utf-8')\n",
    "\n",
    "geology_regional_31_classes_moselle.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Create a dictionary to map permeability classes to corresponding columns\n",
    "permeability_columns = {\n",
    "    \"high\": [\"lit_fra_Alluvium\", 'lit_fra_Coal', 'lit_fra_Conglomerate', 'lit_fra_Gravel and sand',\n",
    "             'lit_fra_Sand', 'lit_fra_Sand and gravel', 'lit_fra_Sandstone and conglomerate', 'lit_fra_Sandstone'\n",
    "        ],\n",
    "    \n",
    "    \"medium\": ['lit_fra_Limestone', 'lit_fra_Sandstone and marl', 'lit_fra_Sandstone and schist',\n",
    "              'lit_fra_Sandstone, conglomerate and marl',\n",
    "\n",
    "              'lit_fra_Arkose', 'lit_fra_Dolomite rock', 'lit_fra_Limestone and marl', 'lit_fra_Marl', \n",
    "             'lit_fra_Marl and dolomite', 'lit_fra_Marl and limestone', 'lit_fra_Marl and sandstone',\n",
    "               'lit_fra_Sandstone and siltstone', 'lit_fra_Sandstone, siltstone and schist', \n",
    "              'lit_fra_Schist and sandstone', 'lit_fra_Silt',  'lit_fra_Silt and schist', 'lit_fra_Siltstone, sandstone and schist'\n",
    "              \n",
    "             ],\n",
    "    \n",
    "    \"low\": ['lit_fra_Cristallin basement', 'lit_fra_Plutonic rock',  'lit_fra_Quarzite',\n",
    "                    'lit_fra_Schist','lit_fra_Volcanic rock' \n",
    "                   ]\n",
    "}\n",
    "\n",
    "# Iterate over the permeability columns and calculate the area for each class\n",
    "for permeability_class, columns in permeability_columns.items():\n",
    "    geology_regional_31_classes_moselle[f'area_perm_{permeability_class}'] = geology_regional_31_classes_moselle[columns].sum(axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "geology_regional_31_classes_moselle = geology_regional_31_classes_moselle[[\"area_perm_high\", \"area_perm_medium\", \"area_perm_low\"]]\n",
    "\n",
    "# Rename the columns\n",
    "geology_regional_31_classes_moselle.columns = [\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "geology_regional_31_classes_moselle\n",
    "\n",
    "geology_regional_31_classes_moselle[\"baseflow_index\"] = estreams_attributes[\"baseflow_index\"]\n",
    "geology_regional_31_classes_moselle.corr(method=\"pearson\")\n",
    "\n",
    "# Concatenation\n",
    "estreams_attributes[[\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]] = geology_regional_31_classes_moselle[[\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]]\n",
    "\n",
    "# Adjust the three categories for also global dataset\n",
    "estreams_attributes[\"perm_high_glob2\"] = estreams_attributes[\"perm_high_glob\"]\n",
    "estreams_attributes[\"perm_medium_glob2\"] = estreams_attributes[\"perm_medium_glob\"] + estreams_attributes[\"perm_low_glob\"]\n",
    "estreams_attributes[\"perm_low_glob2\"] = estreams_attributes[\"perm_verylow_glob\"]\n",
    "\n",
    "###########################################################################################################################\n",
    "# Adjust the columns of the dataset:\n",
    "for basin_id in estreams_attributes.index.tolist():\n",
    "\n",
    "    # Extract and divide by 100\n",
    "    v1 = estreams_attributes.loc[basin_id, \"perm_high_regi\"] / 100\n",
    "    v2 = estreams_attributes.loc[basin_id, \"perm_medium_regi\"] / 100\n",
    "    v3 = estreams_attributes.loc[basin_id, \"perm_low_regi\"] / 100\n",
    "\n",
    "    # Round all values to one decimal place\n",
    "    v1 = round(v1, 2)\n",
    "    v2 = round(v2, 2)\n",
    "    v3 = round(v3, 2)\n",
    "\n",
    "    # Ensure the sum is exactly 1 by adjusting the largest value\n",
    "    diff = 1 - (v1 + v2 + v3)\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the value that was the largest before rounding\n",
    "        if max(v1, v2, v3) == v1:\n",
    "            v1 += diff\n",
    "        elif max(v1, v2, v3) == v2:\n",
    "            v2 += diff\n",
    "        else:\n",
    "            v3 += diff\n",
    "\n",
    "    # Assign back\n",
    "    estreams_attributes.loc[basin_id, \"perm_high_regi\"] = v1 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_medium_regi\"] = v2 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_low_regi\"] = v3 * 100\n",
    "\n",
    "\n",
    "for basin_id in estreams_attributes.index.tolist():\n",
    "\n",
    "    # Extract and divide by 100\n",
    "    v1 = estreams_attributes.loc[basin_id, \"perm_high_glob2\"] / 100\n",
    "    v2 = estreams_attributes.loc[basin_id, \"perm_medium_glob2\"] / 100\n",
    "    v3 = estreams_attributes.loc[basin_id, \"perm_low_glob2\"] / 100\n",
    "\n",
    "    # Round all values to one decimal place\n",
    "    v1 = round(v1, 2)\n",
    "    v2 = round(v2, 2)\n",
    "    v3 = round(v3, 2)\n",
    "\n",
    "    # Ensure the sum is exactly 1 by adjusting the largest value\n",
    "    diff = 1 - (v1 + v2 + v3)\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the value that was the largest before rounding\n",
    "        if max(v1, v2, v3) == v1:\n",
    "            v1 += diff\n",
    "        elif max(v1, v2, v3) == v2:\n",
    "            v2 += diff\n",
    "        else:\n",
    "            v3 += diff\n",
    "\n",
    "    # Assign back\n",
    "    estreams_attributes.loc[basin_id, \"perm_high_glob2\"] = v1 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_medium_glob2\"] = v2 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_low_glob2\"] = v3 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c1bc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions\n",
    "def obj_fun_nsee(observations, simulation, expo=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Squared Error Efficiency (NSEE) while ensuring that\n",
    "    NaNs in simulation are NOT masked (only NaNs in observations are masked).\n",
    "\n",
    "    Parameters:\n",
    "        observations (array-like): Observed values (with fixed NaNs).\n",
    "        simulation (array-like): Simulated values (can contain NaNs).\n",
    "        expo (float, optional): Exponent applied to observations and simulations. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        float: NSEE score (higher values indicate worse performance).\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    simulation = np.asarray(simulation)\n",
    "\n",
    "    # Mask only NaNs in observations\n",
    "    mask = ~np.isnan(observations)\n",
    "    obs = observations[mask]\n",
    "    sim = simulation[mask]  # Keep all simulated values, even NaNs\n",
    "\n",
    "    # If simulation contains NaNs after masking observations, return penalty\n",
    "    if np.isnan(sim).any():\n",
    "        return 10.0  # Large penalty if NaNs appear in the simulation\n",
    "\n",
    "    metric = np.sum((sim**expo - obs**expo)**2) / np.sum((obs**expo - np.mean(obs**expo))**2)\n",
    "    \n",
    "    return float(metric)\n",
    "\n",
    "\n",
    "def obj_fun_kge(observations, simulation):\n",
    "    \"\"\"\n",
    "    Calculate the KGE-2012 objective function, ensuring that NaNs in simulation are NOT masked.\n",
    "    \n",
    "    Parameters:\n",
    "        observations (array-like): Observed values (with fixed NaNs).\n",
    "        simulation (array-like): Simulated values (can contain NaNs).\n",
    "\n",
    "    Returns:\n",
    "        float: KGE-2012 score (higher values indicate worse performance).\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    simulation = np.asarray(simulation)\n",
    "\n",
    "    # Mask only NaNs in observations\n",
    "    mask = ~np.isnan(observations)\n",
    "    obs = observations[mask]\n",
    "    sim = simulation[mask]  # Keep all simulated values, even NaNs\n",
    "\n",
    "    # Check if there are NaNs in the simulation after masking obs\n",
    "    if np.isnan(sim).any():\n",
    "        return 10.0  # Large penalty if the simulation contains NaNs\n",
    "    \n",
    "    obs_mean = np.mean(obs)\n",
    "    sim_mean = np.mean(sim)\n",
    "\n",
    "    r = np.corrcoef(obs, sim)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs)\n",
    "    beta = sim_mean / obs_mean\n",
    "\n",
    "    kge = np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)  # KGE-2012\n",
    "\n",
    "    return float(kge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d064d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 828.42it/s]\n"
     ]
    }
   ],
   "source": [
    "# First we define the outlet of the Moselle to be used\n",
    "outlets = [\"DEBU1959\"]\n",
    "nested_cats_df = nested_catchments.loc[outlets, :]\n",
    "\n",
    "# Now we save our dataframes in a dictionary of dataframes. One dataframe for each watershed. \n",
    "\n",
    "nested_cats_filtered = find_max_unique_rows(nested_cats_df)                                  # Filter only the catchemnts using the function stated before\n",
    "nested_cats_filtered_df = nested_catchments.loc[nested_cats_filtered, :]                     # Here we filter the catchemnts for the list (again, after we apply our function):\n",
    "\n",
    "# Store the variables for the selected catchments in a list of dataframes now for only the ones above 20 cats:\n",
    "estreams_attributes_dfs = {}\n",
    "for catchment in tqdm.tqdm(nested_cats_filtered):\n",
    "    # Retrieve the nested list of catchments for the current catchment\n",
    "    nested_clip = nested_cats_filtered_df.loc[catchment, 'nested_catchments']\n",
    "    \n",
    "    # Filter values to include only those that exist in the index of estreams_attributes\n",
    "    nested_clip = [value for value in nested_clip if value in estreams_attributes.index]\n",
    "    \n",
    "    # Filter the estreams_attributes DataFrame based on the filtered nested_clip\n",
    "    cat_clip = estreams_attributes.loc[nested_clip, :]\n",
    "    \n",
    "    # Store the resulting DataFrame in the dictionary\n",
    "    estreams_attributes_dfs[catchment] = cat_clip\n",
    "\n",
    "# Here we can save the length of each watershed (number of nested catchemnts)\n",
    "catchment_lens = pd.DataFrame(index = estreams_attributes_dfs.keys())\n",
    "for catchment, data in estreams_attributes_dfs.items():\n",
    "    catchment_lens.loc[catchment, \"len\"] = len(data)\n",
    "\n",
    "# Now we can filter it properly:\n",
    "nested_cats_filtered_abovevalue = catchment_lens[catchment_lens.len >= 10]\n",
    "\n",
    "# # Here we filter the catchemnts for the list (again, after we apply our function):\n",
    "nested_cats_filtered_abovevalue_df = nested_catchments.loc[nested_cats_filtered_abovevalue.index, :]\n",
    "\n",
    "# Store the variables for the selected catchments in a list of dataframes now for only the ones above 20 cats:\n",
    "estreams_attributes_dfs = {}\n",
    "\n",
    "for catchment in tqdm.tqdm(nested_cats_filtered_abovevalue_df.index):\n",
    "    # Retrieve the nested list of catchments for the current catchment\n",
    "    nested_clip = nested_cats_filtered_abovevalue_df.loc[catchment, 'nested_catchments']\n",
    "    \n",
    "    # Filter values to include only those that exist in the index of estreams_attributes\n",
    "    nested_clip = [value for value in nested_clip if value in estreams_attributes.index]\n",
    "    \n",
    "    # Filter the estreams_attributes DataFrame based on the filtered nested_clip\n",
    "    cat_clip = estreams_attributes.loc[nested_clip, :]\n",
    "    \n",
    "    # Store the resulting DataFrame in the dictionary\n",
    "    estreams_attributes_dfs[catchment] = cat_clip\n",
    "\n",
    "# Adjust and clip it:\n",
    "estreams_attributes_clipped = estreams_attributes_dfs[\"DEBU1959\"]\n",
    "\n",
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "estreams_attributes_clipped['start_date'] = pd.to_datetime(estreams_attributes_clipped['start_date'])\n",
    "estreams_attributes_clipped['end_date'] = pd.to_datetime(estreams_attributes_clipped['end_date'])\n",
    "\n",
    "\n",
    "#estreams_attributes_clipped_filters = estreams_attributes_clipped[estreams_attributes_clipped.end_date >= \"2010\"]\n",
    "#estreams_attributes_clipped_filters = estreams_attributes_clipped_filters[estreams_attributes_clipped_filters.start_date <= \"2002\"]\n",
    "\n",
    "# Here we retrieve the conectivity (from EStreams computation)\n",
    "# Load the nested catchments CSV file\n",
    "df = pd.read_excel(\"../data/nested_catchments.xlsx\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={df.columns[1]: \"basin_id\", df.columns[2]: \"connected_basin_id\"})\n",
    "df = df.drop(columns=[df.columns[0]])  # Drop the unnamed index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72afebd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>gauge_name</th>\n",
       "      <th>gauge_country</th>\n",
       "      <th>gauge_provider</th>\n",
       "      <th>river</th>\n",
       "      <th>lon_snap</th>\n",
       "      <th>lat_snap</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>irri_1990</th>\n",
       "      <th>irri_2005</th>\n",
       "      <th>stations_num_p_mean</th>\n",
       "      <th>perm_high_regi</th>\n",
       "      <th>perm_medium_regi</th>\n",
       "      <th>perm_low_regi</th>\n",
       "      <th>perm_high_glob2</th>\n",
       "      <th>perm_medium_glob2</th>\n",
       "      <th>perm_low_glob2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LU000018</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Schoenfels</td>\n",
       "      <td>LU</td>\n",
       "      <td>LU_CONTACTFORM</td>\n",
       "      <td>Mamer</td>\n",
       "      <td>6.100795</td>\n",
       "      <td>49.723112</td>\n",
       "      <td>6.100795</td>\n",
       "      <td>49.723112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>17.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000010</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Hunnebuer</td>\n",
       "      <td>LU</td>\n",
       "      <td>LU_CONTACTFORM</td>\n",
       "      <td>Eisch</td>\n",
       "      <td>6.079524</td>\n",
       "      <td>49.729184</td>\n",
       "      <td>6.079524</td>\n",
       "      <td>49.729184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>16.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000001</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>Bigonville</td>\n",
       "      <td>LU</td>\n",
       "      <td>LU_CONTACTFORM</td>\n",
       "      <td>Sure</td>\n",
       "      <td>5.801399</td>\n",
       "      <td>49.869821</td>\n",
       "      <td>5.801399</td>\n",
       "      <td>49.869821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERP2028</th>\n",
       "      <td>3</td>\n",
       "      <td>2674030900</td>\n",
       "      <td>Eisenschmitt</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE_RP</td>\n",
       "      <td>Salm</td>\n",
       "      <td>6.718000</td>\n",
       "      <td>50.048000</td>\n",
       "      <td>6.718000</td>\n",
       "      <td>50.048000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR000183</th>\n",
       "      <td>4</td>\n",
       "      <td>A900105050</td>\n",
       "      <td>A9001050</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Sarre à Laneuveville-lès-Lorquin</td>\n",
       "      <td>7.008689</td>\n",
       "      <td>48.654579</td>\n",
       "      <td>7.008689</td>\n",
       "      <td>48.654579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR003271</th>\n",
       "      <td>107</td>\n",
       "      <td>A782101001</td>\n",
       "      <td>La Seille Ã  Nomeny</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Seille à Nomeny</td>\n",
       "      <td>6.227788</td>\n",
       "      <td>48.888271</td>\n",
       "      <td>6.227788</td>\n",
       "      <td>48.888271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.436</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR003301</th>\n",
       "      <td>108</td>\n",
       "      <td>A930108040</td>\n",
       "      <td>La Sarre Ã  Wittring</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Sarre à Wittring</td>\n",
       "      <td>7.150066</td>\n",
       "      <td>49.053225</td>\n",
       "      <td>7.150066</td>\n",
       "      <td>49.053225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436</td>\n",
       "      <td>2.205</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERP2003</th>\n",
       "      <td>109</td>\n",
       "      <td>2620050500</td>\n",
       "      <td>Bollendorf</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE_RP</td>\n",
       "      <td>Sauer</td>\n",
       "      <td>6.359000</td>\n",
       "      <td>49.851000</td>\n",
       "      <td>6.359000</td>\n",
       "      <td>49.851000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.627</td>\n",
       "      <td>4.160</td>\n",
       "      <td>65.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEBU1958</th>\n",
       "      <td>110</td>\n",
       "      <td>26500100</td>\n",
       "      <td>BundespegelTrierUp</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE_BU</td>\n",
       "      <td>Mosel</td>\n",
       "      <td>6.627000</td>\n",
       "      <td>49.732000</td>\n",
       "      <td>6.627000</td>\n",
       "      <td>49.732000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.862</td>\n",
       "      <td>12.761</td>\n",
       "      <td>219.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR000184</th>\n",
       "      <td>111</td>\n",
       "      <td>A901305050</td>\n",
       "      <td>A9013050</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Sarre Rouge à Vasperviller</td>\n",
       "      <td>7.060836</td>\n",
       "      <td>48.640785</td>\n",
       "      <td>7.064290</td>\n",
       "      <td>48.640085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0    gauge_id            gauge_name gauge_country  \\\n",
       "basin_id                                                               \n",
       "LU000018           0           5            Schoenfels            LU   \n",
       "LU000010           1           6             Hunnebuer            LU   \n",
       "LU000001           2          17            Bigonville            LU   \n",
       "DERP2028           3  2674030900          Eisenschmitt            DE   \n",
       "FR000183           4  A900105050              A9001050            FR   \n",
       "...              ...         ...                   ...           ...   \n",
       "FR003271         107  A782101001   La Seille Ã  Nomeny            FR   \n",
       "FR003301         108  A930108040  La Sarre Ã  Wittring            FR   \n",
       "DERP2003         109  2620050500            Bollendorf            DE   \n",
       "DEBU1958         110    26500100    BundespegelTrierUp            DE   \n",
       "FR000184         111  A901305050              A9013050            FR   \n",
       "\n",
       "          gauge_provider                                river  lon_snap  \\\n",
       "basin_id                                                                  \n",
       "LU000018  LU_CONTACTFORM                                Mamer  6.100795   \n",
       "LU000010  LU_CONTACTFORM                                Eisch  6.079524   \n",
       "LU000001  LU_CONTACTFORM                                 Sure  5.801399   \n",
       "DERP2028           DE_RP                                 Salm  6.718000   \n",
       "FR000183    FR_EAUFRANCE  La Sarre à Laneuveville-lès-Lorquin  7.008689   \n",
       "...                  ...                                  ...       ...   \n",
       "FR003271    FR_EAUFRANCE                   La Seille à Nomeny  6.227788   \n",
       "FR003301    FR_EAUFRANCE                  La Sarre à Wittring  7.150066   \n",
       "DERP2003           DE_RP                                Sauer  6.359000   \n",
       "DEBU1958           DE_BU                                Mosel  6.627000   \n",
       "FR000184    FR_EAUFRANCE        La Sarre Rouge à Vasperviller  7.060836   \n",
       "\n",
       "           lat_snap       lon        lat  ...  irri_1990  irri_2005  \\\n",
       "basin_id                                  ...                         \n",
       "LU000018  49.723112  6.100795  49.723112  ...      0.015      0.015   \n",
       "LU000010  49.729184  6.079524  49.729184  ...      0.026      0.026   \n",
       "LU000001  49.869821  5.801399  49.869821  ...      0.000      0.000   \n",
       "DERP2028  50.048000  6.718000  50.048000  ...      0.000      0.000   \n",
       "FR000183  48.654579  7.008689  48.654579  ...      0.000      0.000   \n",
       "...             ...       ...        ...  ...        ...        ...   \n",
       "FR003271  48.888271  6.227788  48.888271  ...      0.429      0.436   \n",
       "FR003301  49.053225  7.150066  49.053225  ...      0.436      2.205   \n",
       "DERP2003  49.851000  6.359000  49.851000  ...      1.627      4.160   \n",
       "DEBU1958  49.732000  6.627000  49.732000  ...     10.862     12.761   \n",
       "FR000184  48.640785  7.064290  48.640085  ...      0.000      0.000   \n",
       "\n",
       "          stations_num_p_mean  perm_high_regi  perm_medium_regi perm_low_regi  \\\n",
       "basin_id                                                                        \n",
       "LU000018                 17.0            39.0              61.0           0.0   \n",
       "LU000010                 16.0            42.0              58.0           0.0   \n",
       "LU000001                  9.0             1.0               0.0          99.0   \n",
       "DERP2028                 10.0            80.0               7.0          13.0   \n",
       "FR000183                  4.0            66.0              29.0           5.0   \n",
       "...                       ...             ...               ...           ...   \n",
       "FR003271                  5.0             8.0              92.0           0.0   \n",
       "FR003301                 16.0            17.0              83.0           0.0   \n",
       "DERP2003                 65.0            17.0              30.0          53.0   \n",
       "DEBU1958                219.0            25.0              60.0          15.0   \n",
       "FR000184                  5.0            98.0               2.0           0.0   \n",
       "\n",
       "         perm_high_glob2  perm_medium_glob2  perm_low_glob2    group  \n",
       "basin_id                                                              \n",
       "LU000018             0.0              100.0             0.0  Group_1  \n",
       "LU000010             1.0               99.0             0.0  Group_1  \n",
       "LU000001           100.0                0.0             0.0  Group_1  \n",
       "DERP2028            79.0               20.0             1.0  Group_1  \n",
       "FR000183            64.0               30.0             6.0  Group_1  \n",
       "...                  ...                ...             ...      ...  \n",
       "FR003271             0.0              100.0             0.0  Group_7  \n",
       "FR003301            15.0               85.0             0.0  Group_7  \n",
       "DERP2003            50.0               50.0             0.0  Group_7  \n",
       "DEBU1958            28.0               68.0             4.0  Group_7  \n",
       "FR000184            96.0                4.0             0.0  Group_7  \n",
       "\n",
       "[112 rows x 127 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset network\n",
    "estreams_attributes_clipped_filters = pd.read_csv(R'..\\data\\network_estreams_moselle_108_gauges.csv', encoding='utf-8')\n",
    "estreams_attributes_clipped_filters.set_index(\"basin_id\", inplace = True)\n",
    "estreams_attributes_clipped_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5bfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python implementation\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.framework.node import Node\n",
    "from superflexpy.framework.network import Network\n",
    "\n",
    "from superflexpy.implementation.elements.hbv import UnsaturatedReservoir, PowerReservoir\n",
    "\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerPython\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusPython\n",
    "\n",
    "# Numba implementation:\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusNumba\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerNumba\n",
    "\n",
    "from superflexpy.implementation.elements.hbv import PowerReservoir\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.implementation.elements.thur_model_hess import SnowReservoir, UnsaturatedReservoir, PowerReservoir, HalfTriangularLag\n",
    "\n",
    "from superflexpy.implementation.elements.structure_elements import Transparent, Junction, Splitter\n",
    "from superflexpy.framework.element import ParameterizedElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52343368",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_finder = PegasusNumba()\n",
    "num_app = ImplicitEulerNumba(root_finder=root_finder)\n",
    "\n",
    "class ParameterizedSingleFluxSplitter(ParameterizedElement):\n",
    "    _num_downstream = 2\n",
    "    _num_upstream = 1\n",
    "    \n",
    "    def set_input(self, input):\n",
    "\n",
    "        self.input = {'Q_in': input[0]}\n",
    "\n",
    "    def get_output(self, solve=True):\n",
    "\n",
    "        split_par = self._parameters[self._prefix_parameters + 'splitpar']\n",
    "\n",
    "        output1 = [self.input['Q_in'] * split_par]\n",
    "        output2 = [self.input['Q_in'] * (1 - split_par)]\n",
    "        \n",
    "        return [output1, output2]   \n",
    "    \n",
    "    \n",
    "lower_splitter = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.5},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "lower_splitter_medium = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.6},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "lower_splitter_high = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.7},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "# Fluxes in the order P, T, PET\n",
    "upper_splitter = Splitter(\n",
    "    direction=[\n",
    "        [0, 1, None],    # P and T go to the snow reservoir\n",
    "        [2, None, None]  # PET goes to the transparent element\n",
    "    ],\n",
    "    weight=[\n",
    "        [1.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ],\n",
    "    id='upper-splitter'\n",
    ")\n",
    "\n",
    "snow = SnowReservoir(\n",
    "    parameters={'t0': 0.0, 'k': 0.01, 'm': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='snow'\n",
    ")\n",
    "\n",
    "upper_transparent = Transparent(\n",
    "    id='upper-transparent'\n",
    ")\n",
    "\n",
    "upper_junction = Junction(\n",
    "    direction=[\n",
    "        [0, None],\n",
    "        [None, 0]\n",
    "    ],\n",
    "    id='upper-junction'\n",
    ")\n",
    "\n",
    "\n",
    "unsaturated = UnsaturatedReservoir(\n",
    "    parameters={'Smax': 150.0, 'Ce': 1.0, 'm': 0.01, 'beta': 2.0},\n",
    "    states={'S0': 10.0},\n",
    "    approximation=num_app,\n",
    "    id='unsaturated'\n",
    ")\n",
    "\n",
    "fast = PowerReservoir(\n",
    "    parameters={'k': 0.01, 'alpha': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='fast'\n",
    ")\n",
    "\n",
    "slow = PowerReservoir(\n",
    "    parameters={'k': 1e-4, 'alpha': 1.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='slow'\n",
    ")\n",
    "\n",
    "slowhigh = PowerReservoir(\n",
    "    parameters={'k': 1e-4, 'alpha': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='slowhigh'\n",
    ")\n",
    "\n",
    "\n",
    "lower_junction = Junction(\n",
    "    direction=[\n",
    "        [0, 0]\n",
    "    ],\n",
    "    id='lower-junction'\n",
    ")\n",
    "\n",
    "lag_fun = HalfTriangularLag(\n",
    "    parameters={'lag-time': 4.0},\n",
    "    states={'lag': None},\n",
    "    id='lag-fun'\n",
    ")\n",
    "\n",
    "lower_transparent = Transparent(\n",
    "    id='lower-transparent'\n",
    ")\n",
    "\n",
    "lower_transparent2 = Transparent(\n",
    "    id='lower-transparent2'\n",
    ")\n",
    "\n",
    "general = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "    id='general'\n",
    ")\n",
    "\n",
    "low = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [fast],\n",
    "    ],\n",
    "    id='low'\n",
    ")\n",
    "\n",
    "high = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [slowhigh],\n",
    "    ],\n",
    "    id='high'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa00aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Dictionary to store all parameter dicts\n",
    "all_param_dicts = {}\n",
    "\n",
    "# Loop through all CSVs in the current directory\n",
    "for filepath in glob.glob(\"../results/groups/RD_WD/*RD*.csv\"):\n",
    "    file_key = os.path.splitext(os.path.basename(filepath))[0]  # Strip .csv\n",
    "    \n",
    "    param_dict = {}\n",
    "\n",
    "    # Read file and parse lines\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\",\"):  # Skip empty or malformed lines\n",
    "                continue\n",
    "            parts = line.split(\",\")\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                try:\n",
    "                    param_dict[key] = float(value)\n",
    "                except ValueError:\n",
    "                    pass  # Skip lines where value is not a float\n",
    "            else:\n",
    "                pass  # Skip malformed lines\n",
    "\n",
    "    # Store the parsed dictionary\n",
    "    all_param_dicts[file_key] = param_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5307b719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['moselle_best_params_regicompRD_Group_1', 'moselle_best_params_regicompRD_Group_1_2'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_param_dicts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f73ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import numba as nb\n",
    "    from superflexpy.framework.element import ODEsElement\n",
    "    from copy import deepcopy\n",
    "    import numba as nb\n",
    "    from superflexpy.framework.element import ODEsElement\n",
    "    from copy import deepcopy\n",
    "\n",
    "    class CustomUnsaturatedReservoir(UnsaturatedReservoir):\n",
    "        \"\"\"\n",
    "        This class implements the UnsaturatedReservoir of HBV.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(self, parameters, states, approximation, id):\n",
    "            \n",
    "            \"\"\"\n",
    "            This is the initializer of the class UnsaturatedReservoir.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            parameters : dict\n",
    "                Parameters of the element. The keys must be:\n",
    "                'Smax' : maximum reservoir storage\n",
    "                'Ce' : Potential evapotranspiration multiplier\n",
    "                'm' : Smoothing factor for evapotranspiration\n",
    "                'beta' : Exponent in the relation for the streamflow\n",
    "            states : dict\n",
    "                Initial state of the element. The keys must be:\n",
    "                - 'S0' : initial storage of the reservoir.\n",
    "            approximation : superflexpy.utils.numerical_approximation.NumericalApproximator\n",
    "                Numerial method used to approximate the differential equation\n",
    "            id : str\n",
    "                Itentifier of the element. All the elements of the framework must\n",
    "                have an id.\n",
    "            \"\"\"\n",
    "\n",
    "            ODEsElement.__init__(self, parameters=parameters, states=states, approximation=approximation, id=id)\n",
    "\n",
    "            self._fluxes_python = [self._fluxes_function_python]\n",
    "\n",
    "            if approximation.architecture == \"numba\":\n",
    "                self._fluxes = [self._fluxes_function_numba]\n",
    "            elif approximation.architecture == \"python\":\n",
    "                self._fluxes = [self._fluxes_function_python]\n",
    "\n",
    "\n",
    "        # METHODS FOR THE USER\n",
    "\n",
    "        def set_input(self, input):\n",
    "            \"\"\"\n",
    "            Set the input of the element.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            input : list(numpy.ndarray)\n",
    "                List containing the input fluxes of the element. It contains 2\n",
    "                fluxes:\n",
    "                1. Rainfall\n",
    "                2. PET\n",
    "            \"\"\"\n",
    "\n",
    "            self.input = {\"P\": input[0], \"PET\": input[1]}\n",
    "\n",
    "        def get_output(self, solve=True):\n",
    "            \"\"\"\n",
    "            This method solves the differential equation governing the routing\n",
    "            store.\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            list(numpy.ndarray)\n",
    "                Output fluxes in the following order:\n",
    "                1. Streamflow (Q)\n",
    "            \"\"\"\n",
    "\n",
    "            if solve:\n",
    "                self._solver_states = [self._states[self._prefix_states + \"S0\"]]\n",
    "\n",
    "                self._solve_differential_equation()\n",
    "\n",
    "                # Update the state\n",
    "                self.set_states({self._prefix_states + \"S0\": self.state_array[-1, 0]})\n",
    "\n",
    "            fluxes = self._num_app.get_fluxes(\n",
    "                fluxes=self._fluxes_python,\n",
    "                S=self.state_array,\n",
    "                S0=self._solver_states,\n",
    "                dt=self._dt,\n",
    "                **self.input,\n",
    "                **{k[len(self._prefix_parameters) :]: self._parameters[k] for k in self._parameters},\n",
    "            )\n",
    "\n",
    "            return [-fluxes[0][2]]\n",
    "\n",
    "        def get_AET(self):\n",
    "            \"\"\"\n",
    "            This method calculates the actual evapotranspiration\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            numpy.ndarray\n",
    "                Array of actual evapotranspiration\n",
    "            \"\"\"\n",
    "\n",
    "            try:\n",
    "                S = self.state_array\n",
    "            except AttributeError:\n",
    "                message = \"{}get_aet method has to be run after running \".format(self._error_message)\n",
    "                message += \"the model using the method get_output\"\n",
    "                raise AttributeError(message)\n",
    "\n",
    "            fluxes = self._num_app.get_fluxes(\n",
    "                fluxes=self._fluxes_python,\n",
    "                S=S,\n",
    "                S0=self._solver_states,\n",
    "                dt=self._dt,\n",
    "                **self.input,\n",
    "                **{k[len(self._prefix_parameters) :]: self._parameters[k] for k in self._parameters},\n",
    "            )\n",
    "\n",
    "            return [-fluxes[0][1]]\n",
    "\n",
    "        # PROTECTED METHODS\n",
    "\n",
    "        @staticmethod\n",
    "        def _fluxes_function_python(S, S0, ind, P, Csmax, Ce, m, bacon, beta, PET, dt):\n",
    "            # TODO: handle time variable parameters (Smax) -> overflow\n",
    "            Smax = Csmax * bacon\n",
    "\n",
    "            if ind is None:\n",
    "                return (\n",
    "                    [\n",
    "                        P,\n",
    "                        -Ce * PET * ((S / Smax) * (1 + m)) / ((S / Smax) + m),\n",
    "                        -P * (S / Smax) ** beta,\n",
    "                    ],\n",
    "                    0.0,\n",
    "                    S0 + P * dt,\n",
    "                )\n",
    "            else:\n",
    "                Smax[ind] = Csmax[ind] * bacon[ind]\n",
    "\n",
    "                return (\n",
    "                    [\n",
    "                        P[ind],\n",
    "                        -Ce[ind] * PET[ind] * ((S / Smax[ind]) * (1 + m[ind])) / ((S / Smax[ind]) + m[ind]),\n",
    "                        -P[ind] * (S / Smax[ind]) ** beta[ind],\n",
    "                    ],\n",
    "                    0.0,\n",
    "                    S0 + P[ind] * dt[ind],\n",
    "                    [\n",
    "                        0.0,\n",
    "                        -(Ce[ind] * PET[ind] * m[ind] * (m[ind] + 1) * Smax[ind]) / ((S + m[ind] * Smax[ind]) ** 2),\n",
    "                        -(P[ind] * beta[ind] / Smax[ind]) * (S / Smax[ind]) ** (beta[ind] - 1),\n",
    "                    ],\n",
    "                )\n",
    "\n",
    "        @staticmethod\n",
    "        @nb.jit(\n",
    "            \"Tuple((UniTuple(f8, 3), f8, f8,UniTuple(f8, 3)))\"\n",
    "            \"(optional(f8), f8, i4, f8[:], f8[:], f8[:], f8[:], f8[:], f8[:], f8[:], f8[:])\",\n",
    "            nopython=True,\n",
    "        )\n",
    "        def _fluxes_function_numba(S, S0, ind, P, Csmax, Ce, m, bacon, beta, PET, dt):\n",
    "            # TODO: handle time variable parameters (Smax) -> overflow\n",
    "            Smax = Csmax * bacon\n",
    "            \n",
    "            return (\n",
    "                (\n",
    "                    P[ind],\n",
    "                    -Ce[ind] * PET[ind] * ((S / Smax[ind]) * (1 + m[ind])) / ((S / Smax[ind]) + m[ind]),\n",
    "                    -P[ind] * (S / Smax[ind]) ** beta[ind],\n",
    "                ),\n",
    "                0.0,\n",
    "                S0 + P[ind] * dt[ind],\n",
    "                (\n",
    "                    0.0,\n",
    "                    -(Ce[ind] * PET[ind] * m[ind] * (m[ind] + 1) * Smax[ind]) / ((S + m[ind] * Smax[ind]) ** 2),\n",
    "                    -(P[ind] * beta[ind] / Smax[ind]) * (S / Smax[ind]) ** (beta[ind] - 1),\n",
    "                ),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be920d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments_ids = estreams_attributes_clipped_filters.index.tolist()\n",
    "\n",
    "def calculate_hydro_year(date, first_month=10):\n",
    "    \"\"\"\n",
    "    This function calculates the hydrological year from a date. The\n",
    "    hydrological year starts on the month defined by the parameter first_month.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : pandas.core.indexes.datetimes.DatetimeIndex\n",
    "        Date series\n",
    "    first_month : int\n",
    "        Number of the first month of the hydrological year\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Hydrological year time series\n",
    "    \"\"\"\n",
    "\n",
    "    hydrological_year = date.year.values.copy()\n",
    "    hydrological_year[date.month >= first_month] += 1\n",
    "\n",
    "    return hydrological_year\n",
    "\n",
    "def run_model_superflexpy(catchments_ids, best_params_dict_model, perm_areas_model):\n",
    "    # Run the iterative function\n",
    "    iterative_immediate_downstream = find_iterative_immediate_downstream(df, catchments_ids)\n",
    "\n",
    "    # Convert results to a DataFrame for display\n",
    "    iterative_downstream_df = pd.DataFrame(iterative_immediate_downstream.items(), \n",
    "                                        columns=['basin_id', 'immediate_downstream_basin'])\n",
    "\n",
    "\n",
    "    # Assuming the DataFrame has columns 'basin_id' and 'downstream_id'\n",
    "    topology_list = {basin: None for basin in catchments_ids}  # Default to None\n",
    "\n",
    "    # Filter DataFrame for relevant basin_ids and update topology\n",
    "    for _, row in iterative_downstream_df.iterrows():\n",
    "        if row['basin_id'] in topology_list:\n",
    "            topology_list[row['basin_id']] = row['immediate_downstream_basin']\n",
    "\n",
    "    # Generate Nodes dynamically and assign them as global variables\n",
    "    catchments = [] # Dictionary to store nodes\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for cat_id in catchments_ids:\n",
    "\n",
    "        unsaturated = CustomUnsaturatedReservoir(\n",
    "            parameters={'Csmax': 1.5, 'Ce': 0.9509033786507949, 'm': 0.01, 'beta': 0.839150540885298, 'bacon': rootdepth_mean[cat_id]},\n",
    "            states={'S0': 10.0},\n",
    "            approximation=num_app,\n",
    "            id='unsaturated')\n",
    "\n",
    "        general = Unit(\n",
    "            layers=[\n",
    "            [upper_splitter],\n",
    "            [snow, upper_transparent],\n",
    "            [upper_junction],\n",
    "            [unsaturated],\n",
    "            [lower_splitter],\n",
    "            [slow, lag_fun],\n",
    "            [lower_transparent, fast],\n",
    "            [lower_junction],\n",
    "        ],\n",
    "        id='general')\n",
    "\n",
    "        low = Unit(\n",
    "            layers=[\n",
    "            [upper_splitter],\n",
    "            [snow, upper_transparent],\n",
    "            [upper_junction],\n",
    "            [unsaturated],\n",
    "            [lower_splitter],\n",
    "            [slow, lag_fun],\n",
    "            [lower_transparent, fast],\n",
    "            [lower_junction],\n",
    "        ],\n",
    "            id='low')\n",
    "\n",
    "        high = Unit(\n",
    "            layers=[\n",
    "            [upper_splitter],\n",
    "            [snow, upper_transparent],\n",
    "            [upper_junction],\n",
    "            [unsaturated],\n",
    "            [lower_splitter],\n",
    "            [slow, lag_fun],\n",
    "            [lower_transparent, fast],\n",
    "            [lower_junction],\n",
    "        ],\n",
    "            id='high')\n",
    "     \n",
    "        node = Node(\n",
    "            units=[high, general, low],  # Use unit from dictionary or default\n",
    "            weights=perm_areas_model[cat_id],\n",
    "            area=areas.get(cat_id),  # Use predefined area or default\n",
    "            id=cat_id\n",
    "        )\n",
    "        catchments.append(node)  # Store in the list\n",
    "\n",
    "        # Assign the node as a global variable\n",
    "        globals()[cat_id] = node\n",
    "\n",
    "\n",
    "    # Ensure topology only includes nodes that exist in `catchments_ids`\n",
    "    topology = {\n",
    "        cat_id: upstream if upstream in catchments_ids else None\n",
    "        for cat_id, upstream in topology_list.items() if cat_id in catchments_ids\n",
    "    }\n",
    "\n",
    "    # Create the Network\n",
    "    model = Network(\n",
    "        nodes=catchments,  # Pass list of Node objects\n",
    "        topology=topology  \n",
    "    )\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    # Set inputs for each node using the manually defined dictionary\n",
    "    for cat in catchments:\n",
    "        cat.set_input(inputs[cat.id])  # Correct way to set inputs\n",
    "\n",
    "    model.set_timestep(1.0)\n",
    "    model.set_parameters(best_params_dict_model)\n",
    "\n",
    "    output = model.get_output()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def generate_nse_results(catchments_ids, daterange, output, observations, quality_masks):\n",
    "\n",
    "\n",
    "    # Create an empty list to store results\n",
    "    nse_results_cal = []\n",
    "\n",
    "    for basin in catchments_ids:\n",
    "        Qtimeseries = pd.DataFrame(index=daterange)\n",
    "        Qtimeseries[\"Qobs\"] = observations[basin]\n",
    "        Qtimeseries[\"Qcalc\"] = output[basin][0]\n",
    "\n",
    "        hydro_year = calculate_hydro_year(date=Qtimeseries.index, first_month=10)\n",
    "\n",
    "        nse_value = 1 - obj_fun_nsee(observations=Qtimeseries.iloc[365:, 0].values, \n",
    "                                    simulation=Qtimeseries.iloc[365:, 1].values, \n",
    "                                    expo=0.5)\n",
    "                \n",
    "        bfi_obs = hydroanalysis.streamflow_signatures.calculate_baseflow_index(Qtimeseries.iloc[365:, 0].values, quality_masks[basin][365:], alpha=0.925, num_filters=3, num_reflect=30, returnBF=False)\n",
    "        bfi_sim = hydroanalysis.streamflow_signatures.calculate_baseflow_index(Qtimeseries.iloc[365:, 1].values, quality_masks[basin][365:], alpha=0.925, num_filters=3, num_reflect=30, returnBF=False)\n",
    "        \n",
    "        qmean_obs = hydroanalysis.streamflow_signatures.calculate_q_mean(Qtimeseries.iloc[365:, 0].values, quality_masks[basin][365:])\n",
    "        qmean_sim = hydroanalysis.streamflow_signatures.calculate_q_mean(Qtimeseries.iloc[365:, 1].values, quality_masks[basin][365:])\n",
    "        \n",
    "        try:\n",
    "\n",
    "            slope_obs = hydroanalysis.streamflow_signatures.calculate_slope_fdc(Qtimeseries.iloc[365:, 0].values, quality_masks[basin][365:])[\"Sawicz\"]\n",
    "            slope_sim = hydroanalysis.streamflow_signatures.calculate_slope_fdc(Qtimeseries.iloc[365:, 1].values, quality_masks[basin][365:])[\"Sawicz\"]\n",
    "        except: \n",
    "            slope_obs = np.nan\n",
    "            slope_sim = np.nan\n",
    "        \n",
    "        try:\n",
    "            hfd_obs = hydroanalysis.streamflow_signatures.calculate_hfd_mean(Qtimeseries.iloc[365:, 0].values, quality_masks[basin][365:], hydro_year[365:])[\"hfd_mean\"]\n",
    "            hfd_sim = hydroanalysis.streamflow_signatures.calculate_hfd_mean(Qtimeseries.iloc[365:, 1].values, quality_masks[basin][365:], hydro_year[365:])[\"hfd_mean\"]\n",
    "\n",
    "        except:\n",
    "            hfd_obs = np.nan\n",
    "            hfd_sim = np.nan\n",
    "\n",
    "        try:            \n",
    "            nse_value_bfi = 1 - obj_fun_nsee(observations=hydroanalysis.streamflow_signatures.calculate_baseflow_index(Qtimeseries.iloc[365:, 0].values, quality_masks[basin][365:], alpha=0.925, num_filters=3, num_reflect=30, returnBF=True)[1], \n",
    "                                    simulation=hydroanalysis.streamflow_signatures.calculate_baseflow_index(Qtimeseries.iloc[365:, 1].values, quality_masks[basin][365:], alpha=0.925, num_filters=3, num_reflect=30, returnBF=True)[1], \n",
    "                                    expo=0.5)\n",
    "        except:\n",
    "            nse_value_bfi = np.nan\n",
    "\n",
    "\n",
    "        nse_results_cal.append({\n",
    "            \"gauge_name\": network_estreams.loc[basin, \"gauge_name\"],\n",
    "            \"basin\": basin,\n",
    "            \"nse\": nse_value,\n",
    "            \"bfi_obs\": bfi_obs,\n",
    "            \"bfi_sim\":bfi_sim,\n",
    "            \"nse_value_bfi\": nse_value_bfi,\n",
    "            \"qmean_obs\": qmean_obs,\n",
    "            \"qmean_sim\": qmean_sim,\n",
    "            \"slope_obs\": slope_obs,\n",
    "            \"slope_sim\": slope_sim,\n",
    "            \"hfd_obs\": hfd_obs,\n",
    "            \"hfd_sim\": hfd_sim\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    nse_results_df = pd.DataFrame(nse_results_cal)\n",
    "\n",
    "    return nse_results_df\n",
    "\n",
    "import re\n",
    "\n",
    "def is_valid_key(k):\n",
    "    # Exclude keys that end in Group_X_2\n",
    "    return not re.search(r'Group_\\d+_2$', k)\n",
    "\n",
    "def is_valid_key_2(k):\n",
    "    # Include only keys that end in Group_X_2\n",
    "    return re.search(r'Group_\\d+_2$', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a700755",
   "metadata": {},
   "source": [
    "## Model all time-series using all possible combinations of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4d65c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'cat_id' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m output_global_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(regional_keys):\n\u001b[1;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mrun_model_superflexpy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatchments_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcatchments_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbest_params_dict_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_param_dicts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperm_areas_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperm_areas\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     output_regional_dict[key] \u001b[38;5;241m=\u001b[39m output\n",
      "Cell \u001b[1;32mIn[15], line 47\u001b[0m, in \u001b[0;36mrun_model_superflexpy\u001b[1;34m(catchments_ids, best_params_dict_model, perm_areas_model)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Generate Nodes dynamically and assign them as global variables\u001b[39;00m\n\u001b[0;32m     44\u001b[0m catchments \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;66;03m# Dictionary to store nodes\u001b[39;00m\n\u001b[0;32m     46\u001b[0m unsaturated \u001b[38;5;241m=\u001b[39m CustomUnsaturatedReservoir(\n\u001b[1;32m---> 47\u001b[0m     parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCsmax\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCe\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.9509033786507949\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.839150540885298\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbacon\u001b[39m\u001b[38;5;124m'\u001b[39m: rootdepth_mean[\u001b[43mcat_id\u001b[49m]},\n\u001b[0;32m     48\u001b[0m     states\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS0\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10.0\u001b[39m},\n\u001b[0;32m     49\u001b[0m     approximation\u001b[38;5;241m=\u001b[39mnum_app,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsaturated\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m general \u001b[38;5;241m=\u001b[39m Unit(\n\u001b[0;32m     53\u001b[0m layers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     54\u001b[0m     [upper_splitter],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m ],\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     65\u001b[0m low \u001b[38;5;241m=\u001b[39m Unit(\n\u001b[0;32m     66\u001b[0m     layers\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     67\u001b[0m     [upper_splitter],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m ],\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'cat_id' referenced before assignment"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys = [k for k in all_param_dicts if \"regi\" in k and is_valid_key(k)]\n",
    "continental_keys = [k for k in all_param_dicts if \"cont\" in k and is_valid_key(k)]\n",
    "global_keys = [k for k in all_param_dicts if \"glob\" in k and is_valid_key(k)]\n",
    "\n",
    "output_regional_dict = {}\n",
    "output_continental_dict = {}\n",
    "output_global_dict = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c33a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [03:13<00:00, 27.68s/it]\n",
      "100%|██████████| 7/7 [03:00<00:00, 25.82s/it]\n",
      "100%|██████████| 7/7 [03:28<00:00, 29.82s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys_2 = [k for k in all_param_dicts if \"regi\" in k and is_valid_key_2(k)]\n",
    "continental_keys_2 = [k for k in all_param_dicts if \"cont\" in k and is_valid_key_2(k)]\n",
    "global_keys_2 = [k for k in all_param_dicts if \"glob\" in k and is_valid_key_2(k)]\n",
    "\n",
    "output_regional_dict_8801 = {}\n",
    "output_continental_dict_8801 = {}\n",
    "output_global_dict_8801 = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict_8801[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_dict_8801[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_dict_8801[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12a35136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the concatenated data for the complete series analysis\n",
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "observations1 = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "quality_masks1 = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "\n",
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "observations2 = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "quality_masks2 = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "\n",
    "observations_cal = {}\n",
    "\n",
    "for key in observations1.keys():\n",
    "    arr1 = np.atleast_1d(observations1[key])\n",
    "    arr2 = np.atleast_1d(observations2.get(key, np.array([])))\n",
    "\n",
    "    # Always concatenate arrays, even if they contain NaNs or are empty\n",
    "    observations_cal[key] = np.concatenate([arr1, arr2])\n",
    "\n",
    "quality_masks_cal = {}\n",
    "\n",
    "for key in quality_masks1.keys():\n",
    "    arr1 = np.atleast_1d(quality_masks1[key])\n",
    "    arr2 = np.atleast_1d(quality_masks2.get(key, np.array([])))\n",
    "\n",
    "    # Always concatenate arrays, even if they contain NaNs or are empty\n",
    "    quality_masks_cal[key] = np.concatenate([arr1, arr2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40d96c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load both input files\n",
    "path_inputs_1 = '../data/models/input/subset_1988_2001/inputs.npy'\n",
    "path_inputs_2 = '../data/models/input/subset_2001_2015/inputs.npy'\n",
    "\n",
    "inputs1 = np.load(path_inputs_1, allow_pickle=True).item()\n",
    "inputs2 = np.load(path_inputs_2, allow_pickle=True).item()\n",
    "\n",
    "# Initialize new dictionaries\n",
    "precipitation_cal = {}\n",
    "temperature_cal = {}\n",
    "evaporation_cal = {}\n",
    "\n",
    "for key in inputs1.keys():\n",
    "    # Get (P, T, PET) tuples from each period\n",
    "    p1, t1, pet1 = map(np.atleast_1d, inputs1[key])\n",
    "    p2, t2, pet2 = map(np.atleast_1d, inputs2.get(key, ([], [], [])))\n",
    "\n",
    "    # Concatenate and assign\n",
    "    precipitation_cal[key] = np.concatenate([p1, p2])\n",
    "    temperature_cal[key] = np.concatenate([t1, t2])\n",
    "    evaporation_cal[key] = np.concatenate([pet1, pet2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "153e77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_global_dict_cal = {}\n",
    "\n",
    "for param_key in output_global_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_global_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_global_dict[param_key]:\n",
    "            if gauge_id in output_global_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_global_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_global_dict[param_key][gauge_id])\n",
    "                concatenated = np.concatenate([series_8801, series_recent])\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_global_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fec7d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_continental_dict_cal = {}\n",
    "\n",
    "for param_key in output_continental_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_continental_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_continental_dict[param_key]:\n",
    "            if gauge_id in output_continental_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_continental_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_continental_dict[param_key][gauge_id])\n",
    "                concatenated = np.concatenate([series_8801, series_recent])\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_continental_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33f96314",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_regional_dict_cal = {}\n",
    "\n",
    "for param_key in output_regional_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_regional_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_regional_dict[param_key]:\n",
    "            if gauge_id in output_regional_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_regional_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_regional_dict[param_key][gauge_id])\n",
    "                concatenated = np.concatenate([series_8801, series_recent])\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_regional_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21b169f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [01:16<00:00, 10.89s/it]\n",
      "100%|██████████| 7/7 [01:14<00:00, 10.62s/it]\n",
      "100%|██████████| 7/7 [01:03<00:00,  9.07s/it]\n"
     ]
    }
   ],
   "source": [
    "date_range = pd.date_range(\"1988-10-01\", \"2015-09-30\", freq=\"D\")\n",
    "\n",
    "nse_results_global_dict_cal = {}\n",
    "for key in tqdm.tqdm(output_global_dict_cal):\n",
    "    nse_results_global_dict_cal[key] = generate_nse_results(\n",
    "        catchments_ids,\n",
    "        date_range,\n",
    "        output_global_dict_cal[key],\n",
    "        observations_cal,\n",
    "        quality_masks_cal\n",
    "    )\n",
    "    nse_results_global_dict_cal[key].drop_duplicates(subset='basin', keep='first', inplace=True)\n",
    "\n",
    "nse_results_continental_dict_cal = {}\n",
    "for key in tqdm.tqdm(output_continental_dict_cal):\n",
    "    nse_results_continental_dict_cal[key] = generate_nse_results(\n",
    "        catchments_ids,\n",
    "        date_range,\n",
    "        output_continental_dict_cal[key],\n",
    "        observations_cal,\n",
    "        quality_masks_cal\n",
    "    )\n",
    "    nse_results_continental_dict_cal[key].drop_duplicates(subset='basin', keep='first', inplace=True)\n",
    "\n",
    "nse_results_regional_dict_cal = {}\n",
    "for key in tqdm.tqdm(output_regional_dict_cal):\n",
    "    nse_results_regional_dict_cal[key] = generate_nse_results(\n",
    "        catchments_ids,\n",
    "        date_range,\n",
    "        output_regional_dict_cal[key],\n",
    "        observations_cal,\n",
    "        quality_masks_cal\n",
    "    )\n",
    "    nse_results_regional_dict_cal[key].drop_duplicates(subset='basin', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793720bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diff_cal_complete_dict = {}\n",
    "\n",
    "groups_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for number_group in groups_list:\n",
    "\n",
    "    Group_number = f\"Group_\"+str(number_group)\n",
    "\n",
    "    nse_results_cal_r_1_100_complete_cal = nse_results_regional_dict_cal[f\"moselle_best_params_regicomp_Group_{number_group}\"]\n",
    "    nse_results_cal_c_1_100_complete_cal = nse_results_continental_dict_cal[f\"moselle_best_params_contcomp_Group_{number_group}\"]\n",
    "    nse_results_cal_g_1_100_complete_cal = nse_results_global_dict_cal[f\"moselle_best_params_globcomp_Group_{number_group}\"]\n",
    "\n",
    "    list_cal = estreams_attributes_clipped_filters[estreams_attributes_clipped_filters.group == Group_number].index.tolist()\n",
    "\n",
    "    # Remove strings starting with 'LU'\n",
    "    list_cal = [g for g in list_cal if not g.startswith(\"LU\")]\n",
    "\n",
    "\n",
    "    data_diff_cal_complete = pd.DataFrame(data=nse_results_cal_r_1_100_complete_cal.nse - nse_results_cal_c_1_100_complete_cal.nse)\n",
    "    data_diff_cal_complete[\"nse_rg\"] = nse_results_cal_r_1_100_complete_cal.nse - nse_results_cal_g_1_100_complete_cal.nse\n",
    "\n",
    "    data_diff_cal_complete.index = nse_results_cal_c_1_100_complete_cal.basin\n",
    "\n",
    "    data_diff_cal_complete = data_diff_cal_complete.loc[~data_diff_cal_complete.index.isin(list_cal)]\n",
    "\n",
    "    data_diff_cal_complete[\"nse_r1comp\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).nse\n",
    "    data_diff_cal_complete[\"nse_c1comp\"] = nse_results_cal_c_1_100_complete_cal.set_index(\"basin\", inplace = False).nse\n",
    "    data_diff_cal_complete[\"nse_g1comp\"] = nse_results_cal_g_1_100_complete_cal.set_index(\"basin\", inplace = False).nse\n",
    "\n",
    "    data_diff_cal_complete[\"nse_bfi_r1comp\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).nse_value_bfi\n",
    "    data_diff_cal_complete[\"nse_bfi_c1comp\"] = nse_results_cal_c_1_100_complete_cal.set_index(\"basin\", inplace = False).nse_value_bfi\n",
    "    data_diff_cal_complete[\"nse_bfi_g1comp\"] = nse_results_cal_g_1_100_complete_cal.set_index(\"basin\", inplace = False).nse_value_bfi\n",
    "\n",
    "    data_diff_cal_complete[\"bfi_obs\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).bfi_obs\n",
    "    data_diff_cal_complete[\"bfi_r1comp\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).bfi_sim\n",
    "    data_diff_cal_complete[\"bfi_c1comp\"] = nse_results_cal_c_1_100_complete_cal.set_index(\"basin\", inplace = False).bfi_sim\n",
    "    data_diff_cal_complete[\"bfi_g1comp\"] = nse_results_cal_g_1_100_complete_cal.set_index(\"basin\", inplace = False).bfi_sim\n",
    "\n",
    "    data_diff_cal_complete[\"qmean_obs\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).qmean_obs\n",
    "    data_diff_cal_complete[\"qmean_r1comp\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).qmean_sim\n",
    "    data_diff_cal_complete[\"qmean_c1comp\"] = nse_results_cal_c_1_100_complete_cal.set_index(\"basin\", inplace = False).qmean_sim\n",
    "    data_diff_cal_complete[\"qmean_g1comp\"] = nse_results_cal_g_1_100_complete_cal.set_index(\"basin\", inplace = False).qmean_sim\n",
    "\n",
    "    data_diff_cal_complete[\"slope_obs\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).slope_obs\n",
    "    data_diff_cal_complete[\"slope_r1comp\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).slope_sim\n",
    "    data_diff_cal_complete[\"slope_c1comp\"] = nse_results_cal_c_1_100_complete_cal.set_index(\"basin\", inplace = False).slope_sim\n",
    "    data_diff_cal_complete[\"slope_g1comp\"] = nse_results_cal_g_1_100_complete_cal.set_index(\"basin\", inplace = False).slope_sim\n",
    "\n",
    "    data_diff_cal_complete[\"hfd_obs\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).hfd_obs\n",
    "    data_diff_cal_complete[\"hfd_r1comp\"] = nse_results_cal_r_1_100_complete_cal.set_index(\"basin\", inplace = False).hfd_sim\n",
    "    data_diff_cal_complete[\"hfd_c1comp\"] = nse_results_cal_c_1_100_complete_cal.set_index(\"basin\", inplace = False).hfd_sim\n",
    "    data_diff_cal_complete[\"hfd_g1comp\"] = nse_results_cal_g_1_100_complete_cal.set_index(\"basin\", inplace = False).hfd_sim\n",
    "\n",
    "    data_diff_cal_complete_dict[Group_number] = data_diff_cal_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92450bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nse</th>\n",
       "      <th>nse_rg</th>\n",
       "      <th>nse_r1comp</th>\n",
       "      <th>nse_c1comp</th>\n",
       "      <th>nse_g1comp</th>\n",
       "      <th>nse_bfi_r1comp</th>\n",
       "      <th>nse_bfi_c1comp</th>\n",
       "      <th>nse_bfi_g1comp</th>\n",
       "      <th>bfi_obs</th>\n",
       "      <th>bfi_r1comp</th>\n",
       "      <th>...</th>\n",
       "      <th>qmean_c1comp</th>\n",
       "      <th>qmean_g1comp</th>\n",
       "      <th>slope_obs</th>\n",
       "      <th>slope_r1comp</th>\n",
       "      <th>slope_c1comp</th>\n",
       "      <th>slope_g1comp</th>\n",
       "      <th>hfd_obs</th>\n",
       "      <th>hfd_r1comp</th>\n",
       "      <th>hfd_c1comp</th>\n",
       "      <th>hfd_g1comp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BEWA0066</th>\n",
       "      <td>0.020958</td>\n",
       "      <td>0.220831</td>\n",
       "      <td>0.869482</td>\n",
       "      <td>0.848524</td>\n",
       "      <td>0.648650</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.515742</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>...</td>\n",
       "      <td>1.403940</td>\n",
       "      <td>1.367469</td>\n",
       "      <td>3.400558</td>\n",
       "      <td>3.132025</td>\n",
       "      <td>2.845176</td>\n",
       "      <td>1.355581</td>\n",
       "      <td>126.294118</td>\n",
       "      <td>129.196078</td>\n",
       "      <td>132.264706</td>\n",
       "      <td>147.284314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0067</th>\n",
       "      <td>0.024897</td>\n",
       "      <td>0.161909</td>\n",
       "      <td>0.819062</td>\n",
       "      <td>0.794165</td>\n",
       "      <td>0.657153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.446878</td>\n",
       "      <td>0.556423</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488992</td>\n",
       "      <td>1.467804</td>\n",
       "      <td>3.527884</td>\n",
       "      <td>3.119240</td>\n",
       "      <td>2.819743</td>\n",
       "      <td>1.537788</td>\n",
       "      <td>123.437500</td>\n",
       "      <td>129.833333</td>\n",
       "      <td>132.510417</td>\n",
       "      <td>144.072917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0106</th>\n",
       "      <td>0.015476</td>\n",
       "      <td>0.311818</td>\n",
       "      <td>0.805466</td>\n",
       "      <td>0.789989</td>\n",
       "      <td>0.493647</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496805</td>\n",
       "      <td>0.549555</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089221</td>\n",
       "      <td>1.041524</td>\n",
       "      <td>4.063631</td>\n",
       "      <td>3.658118</td>\n",
       "      <td>3.281967</td>\n",
       "      <td>1.208180</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>131.275362</td>\n",
       "      <td>134.050725</td>\n",
       "      <td>155.028986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0107</th>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.216151</td>\n",
       "      <td>0.740788</td>\n",
       "      <td>0.730550</td>\n",
       "      <td>0.524637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.565063</td>\n",
       "      <td>0.545022</td>\n",
       "      <td>...</td>\n",
       "      <td>1.528824</td>\n",
       "      <td>1.483744</td>\n",
       "      <td>3.418299</td>\n",
       "      <td>3.084049</td>\n",
       "      <td>2.942277</td>\n",
       "      <td>1.240413</td>\n",
       "      <td>133.588235</td>\n",
       "      <td>133.176471</td>\n",
       "      <td>135.205882</td>\n",
       "      <td>155.480392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0119</th>\n",
       "      <td>0.020863</td>\n",
       "      <td>0.193143</td>\n",
       "      <td>0.846986</td>\n",
       "      <td>0.826123</td>\n",
       "      <td>0.653843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508101</td>\n",
       "      <td>0.551866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.421508</td>\n",
       "      <td>1.394639</td>\n",
       "      <td>3.336434</td>\n",
       "      <td>3.182945</td>\n",
       "      <td>2.946742</td>\n",
       "      <td>1.478725</td>\n",
       "      <td>123.266667</td>\n",
       "      <td>128.400000</td>\n",
       "      <td>130.833333</td>\n",
       "      <td>144.744444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000015</th>\n",
       "      <td>0.006560</td>\n",
       "      <td>0.271292</td>\n",
       "      <td>0.839303</td>\n",
       "      <td>0.832742</td>\n",
       "      <td>0.568011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516853</td>\n",
       "      <td>0.547512</td>\n",
       "      <td>...</td>\n",
       "      <td>1.068531</td>\n",
       "      <td>1.035687</td>\n",
       "      <td>4.331634</td>\n",
       "      <td>3.615020</td>\n",
       "      <td>3.239272</td>\n",
       "      <td>1.183083</td>\n",
       "      <td>121.769231</td>\n",
       "      <td>126.835165</td>\n",
       "      <td>129.527473</td>\n",
       "      <td>152.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000016</th>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.054118</td>\n",
       "      <td>0.718177</td>\n",
       "      <td>0.706391</td>\n",
       "      <td>0.664059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564940</td>\n",
       "      <td>0.543577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.735196</td>\n",
       "      <td>0.735611</td>\n",
       "      <td>2.005883</td>\n",
       "      <td>2.580323</td>\n",
       "      <td>2.656785</td>\n",
       "      <td>2.916874</td>\n",
       "      <td>133.615385</td>\n",
       "      <td>130.758242</td>\n",
       "      <td>131.351648</td>\n",
       "      <td>128.549451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000017</th>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.016861</td>\n",
       "      <td>0.891089</td>\n",
       "      <td>0.889009</td>\n",
       "      <td>0.874228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572433</td>\n",
       "      <td>0.583327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.935249</td>\n",
       "      <td>0.921441</td>\n",
       "      <td>2.459555</td>\n",
       "      <td>2.813611</td>\n",
       "      <td>2.552611</td>\n",
       "      <td>1.730310</td>\n",
       "      <td>127.307692</td>\n",
       "      <td>130.769231</td>\n",
       "      <td>132.769231</td>\n",
       "      <td>137.219780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000018</th>\n",
       "      <td>0.175054</td>\n",
       "      <td>0.050307</td>\n",
       "      <td>0.710705</td>\n",
       "      <td>0.535651</td>\n",
       "      <td>0.660397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.549133</td>\n",
       "      <td>0.638370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.784171</td>\n",
       "      <td>0.789521</td>\n",
       "      <td>2.305895</td>\n",
       "      <td>1.863001</td>\n",
       "      <td>1.161126</td>\n",
       "      <td>2.907028</td>\n",
       "      <td>127.384615</td>\n",
       "      <td>136.131868</td>\n",
       "      <td>153.472527</td>\n",
       "      <td>127.219780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000019</th>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.126047</td>\n",
       "      <td>0.856370</td>\n",
       "      <td>0.839098</td>\n",
       "      <td>0.730323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492709</td>\n",
       "      <td>0.556492</td>\n",
       "      <td>...</td>\n",
       "      <td>1.196704</td>\n",
       "      <td>1.168738</td>\n",
       "      <td>3.428468</td>\n",
       "      <td>3.131058</td>\n",
       "      <td>2.948049</td>\n",
       "      <td>1.438410</td>\n",
       "      <td>125.846154</td>\n",
       "      <td>128.153846</td>\n",
       "      <td>130.351648</td>\n",
       "      <td>142.373626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               nse    nse_rg  nse_r1comp  nse_c1comp  nse_g1comp  \\\n",
       "basin                                                              \n",
       "BEWA0066  0.020958  0.220831    0.869482    0.848524    0.648650   \n",
       "BEWA0067  0.024897  0.161909    0.819062    0.794165    0.657153   \n",
       "BEWA0106  0.015476  0.311818    0.805466    0.789989    0.493647   \n",
       "BEWA0107  0.010239  0.216151    0.740788    0.730550    0.524637   \n",
       "BEWA0119  0.020863  0.193143    0.846986    0.826123    0.653843   \n",
       "...            ...       ...         ...         ...         ...   \n",
       "LU000015  0.006560  0.271292    0.839303    0.832742    0.568011   \n",
       "LU000016  0.011786  0.054118    0.718177    0.706391    0.664059   \n",
       "LU000017  0.002081  0.016861    0.891089    0.889009    0.874228   \n",
       "LU000018  0.175054  0.050307    0.710705    0.535651    0.660397   \n",
       "LU000019  0.017272  0.126047    0.856370    0.839098    0.730323   \n",
       "\n",
       "          nse_bfi_r1comp  nse_bfi_c1comp  nse_bfi_g1comp   bfi_obs  \\\n",
       "basin                                                                \n",
       "BEWA0066             NaN             NaN             NaN  0.515742   \n",
       "BEWA0067             NaN             NaN             NaN  0.446878   \n",
       "BEWA0106             NaN             NaN             NaN  0.496805   \n",
       "BEWA0107             NaN             NaN             NaN  0.565063   \n",
       "BEWA0119             NaN             NaN             NaN  0.508101   \n",
       "...                  ...             ...             ...       ...   \n",
       "LU000015             NaN             NaN             NaN  0.516853   \n",
       "LU000016             NaN             NaN             NaN  0.564940   \n",
       "LU000017             NaN             NaN             NaN  0.572433   \n",
       "LU000018             NaN             NaN             NaN  0.549133   \n",
       "LU000019             NaN             NaN             NaN  0.492709   \n",
       "\n",
       "          bfi_r1comp  ...  qmean_c1comp  qmean_g1comp  slope_obs  \\\n",
       "basin                 ...                                          \n",
       "BEWA0066    0.567164  ...      1.403940      1.367469   3.400558   \n",
       "BEWA0067    0.556423  ...      1.488992      1.467804   3.527884   \n",
       "BEWA0106    0.549555  ...      1.089221      1.041524   4.063631   \n",
       "BEWA0107    0.545022  ...      1.528824      1.483744   3.418299   \n",
       "BEWA0119    0.551866  ...      1.421508      1.394639   3.336434   \n",
       "...              ...  ...           ...           ...        ...   \n",
       "LU000015    0.547512  ...      1.068531      1.035687   4.331634   \n",
       "LU000016    0.543577  ...      0.735196      0.735611   2.005883   \n",
       "LU000017    0.583327  ...      0.935249      0.921441   2.459555   \n",
       "LU000018    0.638370  ...      0.784171      0.789521   2.305895   \n",
       "LU000019    0.556492  ...      1.196704      1.168738   3.428468   \n",
       "\n",
       "          slope_r1comp  slope_c1comp  slope_g1comp     hfd_obs  hfd_r1comp  \\\n",
       "basin                                                                        \n",
       "BEWA0066      3.132025      2.845176      1.355581  126.294118  129.196078   \n",
       "BEWA0067      3.119240      2.819743      1.537788  123.437500  129.833333   \n",
       "BEWA0106      3.658118      3.281967      1.208180  127.000000  131.275362   \n",
       "BEWA0107      3.084049      2.942277      1.240413  133.588235  133.176471   \n",
       "BEWA0119      3.182945      2.946742      1.478725  123.266667  128.400000   \n",
       "...                ...           ...           ...         ...         ...   \n",
       "LU000015      3.615020      3.239272      1.183083  121.769231  126.835165   \n",
       "LU000016      2.580323      2.656785      2.916874  133.615385  130.758242   \n",
       "LU000017      2.813611      2.552611      1.730310  127.307692  130.769231   \n",
       "LU000018      1.863001      1.161126      2.907028  127.384615  136.131868   \n",
       "LU000019      3.131058      2.948049      1.438410  125.846154  128.153846   \n",
       "\n",
       "          hfd_c1comp  hfd_g1comp  \n",
       "basin                             \n",
       "BEWA0066  132.264706  147.284314  \n",
       "BEWA0067  132.510417  144.072917  \n",
       "BEWA0106  134.050725  155.028986  \n",
       "BEWA0107  135.205882  155.480392  \n",
       "BEWA0119  130.833333  144.744444  \n",
       "...              ...         ...  \n",
       "LU000015  129.527473  152.285714  \n",
       "LU000016  131.351648  128.549451  \n",
       "LU000017  132.769231  137.219780  \n",
       "LU000018  153.472527  127.219780  \n",
       "LU000019  130.351648  142.373626  \n",
       "\n",
       "[108 rows x 24 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to store all DataFrames\n",
    "dfs = list(data_diff_cal_complete_dict.values())\n",
    "\n",
    "# Concatenate all, aligning by index and columns (outer join)\n",
    "all_data = pd.concat(dfs, axis=0)\n",
    "\n",
    "# Group by index and compute mean\n",
    "data_diff_cal_complete = all_data.groupby(all_data.index).mean()\n",
    "\n",
    "data_diff_cal_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21da17",
   "metadata": {},
   "source": [
    "## Space-time validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4be11a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [03:13<00:00, 27.66s/it]\n",
      "100%|██████████| 7/7 [06:19<00:00, 54.18s/it]\n",
      "100%|██████████| 7/7 [03:53<00:00, 33.33s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys = [k for k in all_param_dicts if \"regi\" in k and is_valid_key(k)]\n",
    "continental_keys = [k for k in all_param_dicts if \"cont\" in k and is_valid_key(k)]\n",
    "global_keys = [k for k in all_param_dicts if \"glob\" in k and is_valid_key(k)]\n",
    "\n",
    "output_regional_val_dict = {}\n",
    "output_continental_val_dict = {}\n",
    "output_global_val_dict = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_val_dict[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_val_dict[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_val_dict[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e47dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [05:07<00:00, 43.86s/it]\n",
      "100%|██████████| 7/7 [03:40<00:00, 31.57s/it]\n",
      "100%|██████████| 7/7 [03:48<00:00, 32.59s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys_2 = [k for k in all_param_dicts if \"regi\" in k and is_valid_key_2(k)]\n",
    "continental_keys_2 = [k for k in all_param_dicts if \"cont\" in k and is_valid_key_2(k)]\n",
    "global_keys_2 = [k for k in all_param_dicts if \"glob\" in k and is_valid_key_2(k)]\n",
    "\n",
    "output_regional_dict_0115 = {}\n",
    "output_continental_dict_0115 = {}\n",
    "output_global_dict_0115 = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict_0115[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_dict_0115[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_dict_0115[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13cfe270",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_global_dict_val = {}\n",
    "\n",
    "for param_key in output_global_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_global_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_global_val_dict[param_key]:\n",
    "            if gauge_id in output_global_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_global_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_global_val_dict[param_key][gauge_id])\n",
    "                concatenated = np.concatenate([series_recent, series_8801])\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_global_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "179f1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_continental_dict_val = {}\n",
    "\n",
    "for param_key in output_continental_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_continental_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_continental_val_dict[param_key]:\n",
    "            if gauge_id in output_continental_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_continental_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_continental_val_dict[param_key][gauge_id])\n",
    "                concatenated = np.concatenate([series_recent, series_8801])\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_continental_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8705d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_regional_dict_val = {}\n",
    "\n",
    "for param_key in output_regional_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_regional_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_regional_val_dict[param_key]:\n",
    "            if gauge_id in output_regional_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_regional_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_regional_val_dict[param_key][gauge_id])\n",
    "                concatenated = np.concatenate([series_recent, series_8801])\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_regional_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f81431f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:03<00:00, 17.63s/it]\n",
      "100%|██████████| 7/7 [02:02<00:00, 17.46s/it]\n",
      "100%|██████████| 7/7 [02:16<00:00, 19.55s/it]\n"
     ]
    }
   ],
   "source": [
    "date_range = pd.date_range(\"1988-10-01\", \"2015-09-30\", freq=\"D\")\n",
    "\n",
    "nse_results_global_dict_val = {}\n",
    "for key in tqdm.tqdm(output_global_dict_val):\n",
    "    nse_results_global_dict_val[key] = generate_nse_results(\n",
    "        catchments_ids,\n",
    "        date_range,\n",
    "        output_global_dict_val[key],\n",
    "        observations_cal,\n",
    "        quality_masks_cal\n",
    "    )\n",
    "    nse_results_global_dict_val[key].drop_duplicates(subset='basin', keep='first', inplace=True)\n",
    "\n",
    "nse_results_continental_dict_val = {}\n",
    "for key in tqdm.tqdm(output_continental_dict_val):\n",
    "    nse_results_continental_dict_val[key] = generate_nse_results(\n",
    "        catchments_ids,\n",
    "        date_range,\n",
    "        output_continental_dict_val[key],\n",
    "        observations_cal,\n",
    "        quality_masks_cal\n",
    "    )\n",
    "    nse_results_continental_dict_val[key].drop_duplicates(subset='basin', keep='first', inplace=True)\n",
    "\n",
    "nse_results_regional_dict_val = {}\n",
    "for key in tqdm.tqdm(output_regional_dict_val):\n",
    "    nse_results_regional_dict_val[key] = generate_nse_results(\n",
    "        catchments_ids,\n",
    "        date_range,\n",
    "        output_regional_dict_val[key],\n",
    "        observations_cal,\n",
    "        quality_masks_cal\n",
    "    )\n",
    "    nse_results_regional_dict_val[key].drop_duplicates(subset='basin', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7b0806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_diff_val_complete_dict = {}\n",
    "\n",
    "groups_list = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "for number_group in groups_list:\n",
    "\n",
    "    Group_number = f\"Group_\"+str(number_group)\n",
    "\n",
    "    nse_results_cal_r_1_100_complete_val = nse_results_regional_dict_val[f\"moselle_best_params_regicomp_Group_{number_group}\"]\n",
    "    nse_results_cal_c_1_100_complete_val = nse_results_continental_dict_val[f\"moselle_best_params_contcomp_Group_{number_group}\"]\n",
    "    nse_results_cal_g_1_100_complete_val = nse_results_global_dict_val[f\"moselle_best_params_globcomp_Group_{number_group}\"]\n",
    "\n",
    "    list_cal = estreams_attributes_clipped_filters[estreams_attributes_clipped_filters.group == Group_number].index.tolist()\n",
    "\n",
    "    # Remove strings starting with 'LU'\n",
    "    list_cal = [g for g in list_cal if not g.startswith(\"LU\")]\n",
    "\n",
    "\n",
    "    data_diff_val_complete = pd.DataFrame(data=nse_results_cal_r_1_100_complete_val.nse - nse_results_cal_c_1_100_complete_val.nse)\n",
    "    data_diff_val_complete[\"nse_rg\"] = nse_results_cal_r_1_100_complete_val.nse - nse_results_cal_g_1_100_complete_val.nse\n",
    "\n",
    "    data_diff_val_complete.index = nse_results_cal_c_1_100_complete_val.basin\n",
    "\n",
    "    data_diff_val_complete = data_diff_val_complete.loc[~data_diff_val_complete.index.isin(list_cal)]\n",
    "\n",
    "    data_diff_val_complete[\"nse_r1comp\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).nse\n",
    "    data_diff_val_complete[\"nse_c1comp\"] = nse_results_cal_c_1_100_complete_val.set_index(\"basin\", inplace = False).nse\n",
    "    data_diff_val_complete[\"nse_g1comp\"] = nse_results_cal_g_1_100_complete_val.set_index(\"basin\", inplace = False).nse\n",
    "\n",
    "    data_diff_val_complete[\"nse_bfi_r1comp\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).nse_value_bfi\n",
    "    data_diff_val_complete[\"nse_bfi_c1comp\"] = nse_results_cal_c_1_100_complete_val.set_index(\"basin\", inplace = False).nse_value_bfi\n",
    "    data_diff_val_complete[\"nse_bfi_g1comp\"] = nse_results_cal_g_1_100_complete_val.set_index(\"basin\", inplace = False).nse_value_bfi\n",
    "\n",
    "    data_diff_val_complete[\"bfi_obs\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).bfi_obs\n",
    "    data_diff_val_complete[\"bfi_r1comp\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).bfi_sim\n",
    "    data_diff_val_complete[\"bfi_c1comp\"] = nse_results_cal_c_1_100_complete_val.set_index(\"basin\", inplace = False).bfi_sim\n",
    "    data_diff_val_complete[\"bfi_g1comp\"] = nse_results_cal_g_1_100_complete_val.set_index(\"basin\", inplace = False).bfi_sim\n",
    "\n",
    "    data_diff_val_complete[\"qmean_obs\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).qmean_obs\n",
    "    data_diff_val_complete[\"qmean_r1comp\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).qmean_sim\n",
    "    data_diff_val_complete[\"qmean_c1comp\"] = nse_results_cal_c_1_100_complete_val.set_index(\"basin\", inplace = False).qmean_sim\n",
    "    data_diff_val_complete[\"qmean_g1comp\"] = nse_results_cal_g_1_100_complete_val.set_index(\"basin\", inplace = False).qmean_sim\n",
    "\n",
    "    data_diff_val_complete[\"slope_obs\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).slope_obs\n",
    "    data_diff_val_complete[\"slope_r1comp\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).slope_sim\n",
    "    data_diff_val_complete[\"slope_c1comp\"] = nse_results_cal_c_1_100_complete_val.set_index(\"basin\", inplace = False).slope_sim\n",
    "    data_diff_val_complete[\"slope_g1comp\"] = nse_results_cal_g_1_100_complete_val.set_index(\"basin\", inplace = False).slope_sim\n",
    "\n",
    "    data_diff_val_complete[\"hfd_obs\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).hfd_obs\n",
    "    data_diff_val_complete[\"hfd_r1comp\"] = nse_results_cal_r_1_100_complete_val.set_index(\"basin\", inplace = False).hfd_sim\n",
    "    data_diff_val_complete[\"hfd_c1comp\"] = nse_results_cal_c_1_100_complete_val.set_index(\"basin\", inplace = False).hfd_sim\n",
    "    data_diff_val_complete[\"hfd_g1comp\"] = nse_results_cal_g_1_100_complete_val.set_index(\"basin\", inplace = False).hfd_sim\n",
    "\n",
    "    data_diff_val_complete_dict[Group_number] = data_diff_val_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f218ac92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nse</th>\n",
       "      <th>nse_rg</th>\n",
       "      <th>nse_r1comp</th>\n",
       "      <th>nse_c1comp</th>\n",
       "      <th>nse_g1comp</th>\n",
       "      <th>nse_bfi_r1comp</th>\n",
       "      <th>nse_bfi_c1comp</th>\n",
       "      <th>nse_bfi_g1comp</th>\n",
       "      <th>bfi_obs</th>\n",
       "      <th>bfi_r1comp</th>\n",
       "      <th>...</th>\n",
       "      <th>qmean_c1comp</th>\n",
       "      <th>qmean_g1comp</th>\n",
       "      <th>slope_obs</th>\n",
       "      <th>slope_r1comp</th>\n",
       "      <th>slope_c1comp</th>\n",
       "      <th>slope_g1comp</th>\n",
       "      <th>hfd_obs</th>\n",
       "      <th>hfd_r1comp</th>\n",
       "      <th>hfd_c1comp</th>\n",
       "      <th>hfd_g1comp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BEWA0066</th>\n",
       "      <td>0.022037</td>\n",
       "      <td>0.220675</td>\n",
       "      <td>0.849026</td>\n",
       "      <td>0.826988</td>\n",
       "      <td>0.628351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.515742</td>\n",
       "      <td>0.569535</td>\n",
       "      <td>...</td>\n",
       "      <td>1.303524</td>\n",
       "      <td>1.285556</td>\n",
       "      <td>3.400558</td>\n",
       "      <td>3.334363</td>\n",
       "      <td>2.987921</td>\n",
       "      <td>1.534966</td>\n",
       "      <td>126.294118</td>\n",
       "      <td>130.764706</td>\n",
       "      <td>134.470588</td>\n",
       "      <td>150.196078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0067</th>\n",
       "      <td>0.031259</td>\n",
       "      <td>0.159491</td>\n",
       "      <td>0.787471</td>\n",
       "      <td>0.756212</td>\n",
       "      <td>0.627980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.446878</td>\n",
       "      <td>0.557149</td>\n",
       "      <td>...</td>\n",
       "      <td>1.350700</td>\n",
       "      <td>1.367239</td>\n",
       "      <td>3.527884</td>\n",
       "      <td>3.338369</td>\n",
       "      <td>2.934753</td>\n",
       "      <td>1.701058</td>\n",
       "      <td>123.437500</td>\n",
       "      <td>132.052083</td>\n",
       "      <td>135.979167</td>\n",
       "      <td>147.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0106</th>\n",
       "      <td>0.013167</td>\n",
       "      <td>0.300600</td>\n",
       "      <td>0.778157</td>\n",
       "      <td>0.764990</td>\n",
       "      <td>0.477557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.496805</td>\n",
       "      <td>0.553363</td>\n",
       "      <td>...</td>\n",
       "      <td>1.071827</td>\n",
       "      <td>1.031436</td>\n",
       "      <td>4.063631</td>\n",
       "      <td>3.783713</td>\n",
       "      <td>3.402966</td>\n",
       "      <td>1.475382</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>131.637681</td>\n",
       "      <td>134.557971</td>\n",
       "      <td>154.724638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0107</th>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.217873</td>\n",
       "      <td>0.743017</td>\n",
       "      <td>0.732049</td>\n",
       "      <td>0.525144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.565063</td>\n",
       "      <td>0.544747</td>\n",
       "      <td>...</td>\n",
       "      <td>1.484112</td>\n",
       "      <td>1.447515</td>\n",
       "      <td>3.418299</td>\n",
       "      <td>3.297442</td>\n",
       "      <td>3.107328</td>\n",
       "      <td>1.405324</td>\n",
       "      <td>133.588235</td>\n",
       "      <td>134.235294</td>\n",
       "      <td>137.137255</td>\n",
       "      <td>157.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BEWA0119</th>\n",
       "      <td>0.019381</td>\n",
       "      <td>0.184988</td>\n",
       "      <td>0.820006</td>\n",
       "      <td>0.800625</td>\n",
       "      <td>0.635017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.508101</td>\n",
       "      <td>0.549784</td>\n",
       "      <td>...</td>\n",
       "      <td>1.379944</td>\n",
       "      <td>1.362483</td>\n",
       "      <td>3.336434</td>\n",
       "      <td>3.442993</td>\n",
       "      <td>3.121945</td>\n",
       "      <td>1.591426</td>\n",
       "      <td>123.266667</td>\n",
       "      <td>129.522222</td>\n",
       "      <td>132.544444</td>\n",
       "      <td>147.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000015</th>\n",
       "      <td>0.012764</td>\n",
       "      <td>0.274970</td>\n",
       "      <td>0.829616</td>\n",
       "      <td>0.816852</td>\n",
       "      <td>0.554646</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.516853</td>\n",
       "      <td>0.549093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.921210</td>\n",
       "      <td>0.922831</td>\n",
       "      <td>4.331634</td>\n",
       "      <td>3.588753</td>\n",
       "      <td>3.119963</td>\n",
       "      <td>1.257188</td>\n",
       "      <td>121.769231</td>\n",
       "      <td>129.021978</td>\n",
       "      <td>132.263736</td>\n",
       "      <td>154.109890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000016</th>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.018681</td>\n",
       "      <td>0.664423</td>\n",
       "      <td>0.661448</td>\n",
       "      <td>0.645742</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.564940</td>\n",
       "      <td>0.565974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.638785</td>\n",
       "      <td>0.664747</td>\n",
       "      <td>2.005883</td>\n",
       "      <td>2.459098</td>\n",
       "      <td>2.490300</td>\n",
       "      <td>2.902758</td>\n",
       "      <td>133.615385</td>\n",
       "      <td>131.153846</td>\n",
       "      <td>131.736264</td>\n",
       "      <td>132.879121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000017</th>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.853616</td>\n",
       "      <td>0.851334</td>\n",
       "      <td>0.823115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.572433</td>\n",
       "      <td>0.588918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804938</td>\n",
       "      <td>0.824275</td>\n",
       "      <td>2.459555</td>\n",
       "      <td>2.802570</td>\n",
       "      <td>2.520245</td>\n",
       "      <td>1.819491</td>\n",
       "      <td>127.307692</td>\n",
       "      <td>132.219780</td>\n",
       "      <td>134.648352</td>\n",
       "      <td>141.747253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000018</th>\n",
       "      <td>0.158776</td>\n",
       "      <td>0.024249</td>\n",
       "      <td>0.639488</td>\n",
       "      <td>0.480712</td>\n",
       "      <td>0.615239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.549133</td>\n",
       "      <td>0.648631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674125</td>\n",
       "      <td>0.714856</td>\n",
       "      <td>2.305895</td>\n",
       "      <td>1.842248</td>\n",
       "      <td>1.165371</td>\n",
       "      <td>2.898113</td>\n",
       "      <td>127.384615</td>\n",
       "      <td>136.318681</td>\n",
       "      <td>152.263736</td>\n",
       "      <td>132.241758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000019</th>\n",
       "      <td>0.018821</td>\n",
       "      <td>0.130592</td>\n",
       "      <td>0.831222</td>\n",
       "      <td>0.812402</td>\n",
       "      <td>0.700630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.492709</td>\n",
       "      <td>0.554720</td>\n",
       "      <td>...</td>\n",
       "      <td>1.022765</td>\n",
       "      <td>1.035007</td>\n",
       "      <td>3.428468</td>\n",
       "      <td>3.251328</td>\n",
       "      <td>2.967959</td>\n",
       "      <td>1.556527</td>\n",
       "      <td>125.846154</td>\n",
       "      <td>131.307692</td>\n",
       "      <td>134.274725</td>\n",
       "      <td>146.989011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               nse    nse_rg  nse_r1comp  nse_c1comp  nse_g1comp  \\\n",
       "basin                                                              \n",
       "BEWA0066  0.022037  0.220675    0.849026    0.826988    0.628351   \n",
       "BEWA0067  0.031259  0.159491    0.787471    0.756212    0.627980   \n",
       "BEWA0106  0.013167  0.300600    0.778157    0.764990    0.477557   \n",
       "BEWA0107  0.010968  0.217873    0.743017    0.732049    0.525144   \n",
       "BEWA0119  0.019381  0.184988    0.820006    0.800625    0.635017   \n",
       "...            ...       ...         ...         ...         ...   \n",
       "LU000015  0.012764  0.274970    0.829616    0.816852    0.554646   \n",
       "LU000016  0.002975  0.018681    0.664423    0.661448    0.645742   \n",
       "LU000017  0.002282  0.030501    0.853616    0.851334    0.823115   \n",
       "LU000018  0.158776  0.024249    0.639488    0.480712    0.615239   \n",
       "LU000019  0.018821  0.130592    0.831222    0.812402    0.700630   \n",
       "\n",
       "          nse_bfi_r1comp  nse_bfi_c1comp  nse_bfi_g1comp   bfi_obs  \\\n",
       "basin                                                                \n",
       "BEWA0066             NaN             NaN             NaN  0.515742   \n",
       "BEWA0067             NaN             NaN             NaN  0.446878   \n",
       "BEWA0106             NaN             NaN             NaN  0.496805   \n",
       "BEWA0107             NaN             NaN             NaN  0.565063   \n",
       "BEWA0119             NaN             NaN             NaN  0.508101   \n",
       "...                  ...             ...             ...       ...   \n",
       "LU000015             NaN             NaN             NaN  0.516853   \n",
       "LU000016             NaN             NaN             NaN  0.564940   \n",
       "LU000017             NaN             NaN             NaN  0.572433   \n",
       "LU000018             NaN             NaN             NaN  0.549133   \n",
       "LU000019             NaN             NaN             NaN  0.492709   \n",
       "\n",
       "          bfi_r1comp  ...  qmean_c1comp  qmean_g1comp  slope_obs  \\\n",
       "basin                 ...                                          \n",
       "BEWA0066    0.569535  ...      1.303524      1.285556   3.400558   \n",
       "BEWA0067    0.557149  ...      1.350700      1.367239   3.527884   \n",
       "BEWA0106    0.553363  ...      1.071827      1.031436   4.063631   \n",
       "BEWA0107    0.544747  ...      1.484112      1.447515   3.418299   \n",
       "BEWA0119    0.549784  ...      1.379944      1.362483   3.336434   \n",
       "...              ...  ...           ...           ...        ...   \n",
       "LU000015    0.549093  ...      0.921210      0.922831   4.331634   \n",
       "LU000016    0.565974  ...      0.638785      0.664747   2.005883   \n",
       "LU000017    0.588918  ...      0.804938      0.824275   2.459555   \n",
       "LU000018    0.648631  ...      0.674125      0.714856   2.305895   \n",
       "LU000019    0.554720  ...      1.022765      1.035007   3.428468   \n",
       "\n",
       "          slope_r1comp  slope_c1comp  slope_g1comp     hfd_obs  hfd_r1comp  \\\n",
       "basin                                                                        \n",
       "BEWA0066      3.334363      2.987921      1.534966  126.294118  130.764706   \n",
       "BEWA0067      3.338369      2.934753      1.701058  123.437500  132.052083   \n",
       "BEWA0106      3.783713      3.402966      1.475382  127.000000  131.637681   \n",
       "BEWA0107      3.297442      3.107328      1.405324  133.588235  134.235294   \n",
       "BEWA0119      3.442993      3.121945      1.591426  123.266667  129.522222   \n",
       "...                ...           ...           ...         ...         ...   \n",
       "LU000015      3.588753      3.119963      1.257188  121.769231  129.021978   \n",
       "LU000016      2.459098      2.490300      2.902758  133.615385  131.153846   \n",
       "LU000017      2.802570      2.520245      1.819491  127.307692  132.219780   \n",
       "LU000018      1.842248      1.165371      2.898113  127.384615  136.318681   \n",
       "LU000019      3.251328      2.967959      1.556527  125.846154  131.307692   \n",
       "\n",
       "          hfd_c1comp  hfd_g1comp  \n",
       "basin                             \n",
       "BEWA0066  134.470588  150.196078  \n",
       "BEWA0067  135.979167  147.708333  \n",
       "BEWA0106  134.557971  154.724638  \n",
       "BEWA0107  137.137255  157.235294  \n",
       "BEWA0119  132.544444  147.388889  \n",
       "...              ...         ...  \n",
       "LU000015  132.263736  154.109890  \n",
       "LU000016  131.736264  132.879121  \n",
       "LU000017  134.648352  141.747253  \n",
       "LU000018  152.263736  132.241758  \n",
       "LU000019  134.274725  146.989011  \n",
       "\n",
       "[108 rows x 24 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List to store all DataFrames\n",
    "dfs = list(data_diff_val_complete_dict.values())\n",
    "\n",
    "# Concatenate all, aligning by index and columns (outer join)\n",
    "all_data = pd.concat(dfs, axis=0)\n",
    "\n",
    "# Group by index and compute mean\n",
    "data_diff_val_complete = all_data.groupby(all_data.index).mean()\n",
    "\n",
    "data_diff_val_complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d195b",
   "metadata": {},
   "source": [
    "## Save the time-series in netcdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4d218e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Adjust these according to your data\n",
    "group_suffixes = [\"Group_1\", \"Group_2\", \"Group_3\", \"Group_4\", \"Group_5\", \"Group_6\", \"Group_7\"]\n",
    "\n",
    "gauge_ids = list(observations_cal.keys())  # Your observations dict should be preloaded\n",
    "time_index = pd.date_range(start=\"1988-10-01\", end=\"2015-09-30\", freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2dd220b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_inputs = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"observation\": ([\"gauge_id\", \"date\"], [observations_cal[g] for g in gauge_ids]),\n",
    "        \"precipitation\": ([\"gauge_id\", \"date\"], [precipitation_cal[g] for g in gauge_ids]),\n",
    "        \"temperature\": ([\"gauge_id\", \"date\"], [temperature_cal[g] for g in gauge_ids]),\n",
    "        \"evaporation\": ([\"gauge_id\", \"date\"], [evaporation_cal[g] for g in gauge_ids]),\n",
    "    },\n",
    "    coords={\n",
    "        \"gauge_id\": gauge_ids,\n",
    "        \"date\": time_index\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_inputs.to_netcdf(rf\"C:\\Users\\nascimth\\OneDrive - Eawag\\Eawag\\Papers\\Moselle_superflexpy\\GitHub\\estreams_superflexpy\\results\\sim\\space-time\\inputs.nc\", engine=\"scipy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31e036cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets\n",
    "datasets = {}\n",
    "\n",
    "for suffix in group_suffixes:\n",
    "    reg_key = f\"moselle_best_params_regicomp_{suffix}\"\n",
    "    cont_key = f\"moselle_best_params_contcomp_{suffix}\"\n",
    "    glob_key = f\"moselle_best_params_globcomp_{suffix}\"\n",
    "\n",
    "    reg_data = []\n",
    "    cont_data = []\n",
    "    glob_data = []\n",
    "    group_gauge_ids = []\n",
    "\n",
    "    for gauge in gauge_ids:\n",
    "        if gauge in output_regional_dict_val[reg_key] and \\\n",
    "           gauge in output_continental_dict_val[cont_key] and \\\n",
    "           gauge in output_global_dict_val[glob_key]:\n",
    "\n",
    "            reg_data.append(output_regional_dict_val[reg_key][gauge][0])\n",
    "            cont_data.append(output_continental_dict_val[cont_key][gauge][0])\n",
    "            glob_data.append(output_global_dict_val[glob_key][gauge][0])\n",
    "            group_gauge_ids.append(gauge)\n",
    "\n",
    "    if group_gauge_ids:\n",
    "        ds = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"regional\": ([\"gauge_id\", \"date\"], reg_data),\n",
    "                \"continental\": ([\"gauge_id\", \"date\"], cont_data),\n",
    "                \"global\": ([\"gauge_id\", \"date\"], glob_data)\n",
    "            },\n",
    "            coords={\n",
    "                \"gauge_id\": group_gauge_ids,\n",
    "                \"date\": time_index\n",
    "            }\n",
    "        )\n",
    "        datasets[suffix] = ds\n",
    "\n",
    "# Save each group to a separate NetCDF file using scipy (no need for netCDF4)\n",
    "for suffix, ds in datasets.items():\n",
    "    ds.to_netcdf(rf\"C:\\Users\\nascimth\\OneDrive - Eawag\\Eawag\\Papers\\Moselle_superflexpy\\GitHub\\estreams_superflexpy\\results\\sim\\space-time\\simu_compl_{suffix}.nc\", engine=\"scipy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb90cb",
   "metadata": {},
   "source": [
    "# Save the performances in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2729b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference dataframe\n",
    "for name, df in data_diff_cal_complete_dict.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space\\{name}_comp.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9f10e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSEs\n",
    "for name, df in nse_results_regional_dict_cal.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space\\nses\\{name}.csv\", index=True)\n",
    "\n",
    "for name, df in nse_results_continental_dict_cal.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space\\nses\\{name}.csv\", index=True)\n",
    "\n",
    "for name, df in nse_results_global_dict_cal.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space\\nses\\{name}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70beb1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference dataframe\n",
    "for name, df in data_diff_val_complete_dict.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space_time\\{name}_comp.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89bef88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSEs\n",
    "for name, df in nse_results_regional_dict_val.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space_time\\nses\\{name}.csv\", index=True)\n",
    "\n",
    "for name, df in nse_results_continental_dict_val.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space_time\\nses\\{name}.csv\", index=True)\n",
    "\n",
    "for name, df in nse_results_global_dict_val.items():\n",
    "    df.to_csv(rf\"..\\results\\performances\\space_time\\nses\\{name}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ac82e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

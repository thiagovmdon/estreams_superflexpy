{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "068701ec",
   "metadata": {},
   "source": [
    "## Part C: Compute evaluation performances \n",
    "\n",
    "This notebook contains all the code used to compute the evaluation performances (space and space-time validation) of the models calibrated using the three geology maps to infer their respective HRUs. \n",
    "\n",
    "\n",
    "Alongside the other notebooks, it covers all the analysis performed in: \"How landscape data quality affects our perception of dominant processes in large-sample hydrology studies?\" paper by do Nascimento et al. (in review). To be able to run this notebook, please ensure that you have downloaded the acompanying data of the paper. All links can be found in the data section of the paper.\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbd754",
   "metadata": {},
   "source": [
    "# Import the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5becc299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spotpy\n",
    "import time\n",
    "import os\n",
    "import tqdm as tqdm\n",
    "import hydroanalysis\n",
    "from utils.functions import find_max_unique_rows\n",
    "from utils.functions import find_iterative_immediate_downstream\n",
    "import geopandas as gpd\n",
    "import re\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276cde8c",
   "metadata": {},
   "source": [
    "## Set the path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0efd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where the EStreams dataset is stored\n",
    "# Eawag\n",
    "path_estreams = r'/Users/nascimth/Documents/data/EStreams'\n",
    "\n",
    "## Mac\n",
    "#path_estreams = r'/Users/thiagomedeirosdonascimento/Downloads/Python 2/Scripts/estreams_part_b/data/EStreams'\n",
    "\n",
    "path_data = r\"/Users/nascimth/Documents/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cfe0e2",
   "metadata": {},
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64ca750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset network\n",
    "network_estreams = pd.read_csv(path_estreams+'/streamflow_gauges/estreams_gauging_stations.csv', encoding='utf-8')\n",
    "network_estreams.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "network_estreams['start_date'] = pd.to_datetime(network_estreams['start_date'])\n",
    "network_estreams['end_date'] = pd.to_datetime(network_estreams['end_date'])\n",
    "\n",
    "# Convert to list both the nested_catchments and the duplicated_suspect columns\n",
    "network_estreams['nested_catchments'] = network_estreams['nested_catchments'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Remove the brackets and handle NaN values\n",
    "network_estreams['duplicated_suspect'] = network_estreams['duplicated_suspect'].apply(\n",
    "    lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x)\n",
    "\n",
    "# Set the nested catchments as a dataframe\n",
    "nested_catchments = pd.DataFrame(network_estreams['nested_catchments'])\n",
    "\n",
    "# Now we add the outlet to the list (IF it was not before):\n",
    "# Ensure that the basin_id is in the nested_catchments\n",
    "for basin_id in nested_catchments.index:\n",
    "    if basin_id not in nested_catchments.at[basin_id, 'nested_catchments']:\n",
    "        nested_catchments.at[basin_id, 'nested_catchments'].append(basin_id)\n",
    "\n",
    "\n",
    "\n",
    "# Attributes already filtered previously:\n",
    "#estreams_attributes = pd.read_csv('data/exploration/estreams_attributes_filtered_moselle_sm_su_tog.csv', encoding='utf-8')\n",
    "estreams_attributes = pd.read_csv('../data/estreams_attributes_filtered_quality_geology_v01.csv', encoding='utf-8')\n",
    "\n",
    "estreams_attributes.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Convert to list both the nested_catchments and the duplicated_suspect columns\n",
    "estreams_attributes['nested_catchments'] = estreams_attributes['nested_catchments'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Remove the brackets and handle NaN values\n",
    "estreams_attributes['duplicated_suspect'] = estreams_attributes['duplicated_suspect'].apply(\n",
    "    lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x)\n",
    "\n",
    "estreams_attributes.sort_index(inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d63e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geological attributes (regional scale)\n",
    "geology_regional_31_classes_moselle = pd.read_csv(\"../data/estreams_geology_moselle_regional_attributes.csv\", encoding='utf-8')\n",
    "\n",
    "geology_regional_31_classes_moselle.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Create a dictionary to map permeability classes to corresponding columns\n",
    "permeability_columns = {\n",
    "    \"high\": [\"lit_fra_Alluvium\", 'lit_fra_Coal', 'lit_fra_Conglomerate', 'lit_fra_Gravel and sand',\n",
    "             'lit_fra_Sand', 'lit_fra_Sand and gravel', 'lit_fra_Sandstone and conglomerate', 'lit_fra_Sandstone'\n",
    "        ],\n",
    "    \n",
    "    \"medium\": ['lit_fra_Limestone', 'lit_fra_Sandstone and marl', 'lit_fra_Sandstone and schist',\n",
    "              'lit_fra_Sandstone, conglomerate and marl',\n",
    "\n",
    "              'lit_fra_Arkose', 'lit_fra_Dolomite rock', 'lit_fra_Limestone and marl', 'lit_fra_Marl', \n",
    "             'lit_fra_Marl and dolomite', 'lit_fra_Marl and limestone', 'lit_fra_Marl and sandstone',\n",
    "               'lit_fra_Sandstone and siltstone', 'lit_fra_Sandstone, siltstone and schist', \n",
    "              'lit_fra_Schist and sandstone', 'lit_fra_Silt',  'lit_fra_Silt and schist', 'lit_fra_Siltstone, sandstone and schist'\n",
    "              \n",
    "             ],\n",
    "    \n",
    "    \"low\": ['lit_fra_Cristallin basement', 'lit_fra_Plutonic rock',  'lit_fra_Quarzite',\n",
    "                    'lit_fra_Schist','lit_fra_Volcanic rock' \n",
    "                   ]\n",
    "}\n",
    "\n",
    "# Iterate over the permeability columns and calculate the area for each class\n",
    "for permeability_class, columns in permeability_columns.items():\n",
    "    geology_regional_31_classes_moselle[f'area_perm_{permeability_class}'] = geology_regional_31_classes_moselle[columns].sum(axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "geology_regional_31_classes_moselle = geology_regional_31_classes_moselle[[\"area_perm_high\", \"area_perm_medium\", \"area_perm_low\"]]\n",
    "\n",
    "# Rename the columns\n",
    "geology_regional_31_classes_moselle.columns = [\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "geology_regional_31_classes_moselle\n",
    "\n",
    "geology_regional_31_classes_moselle[\"baseflow_index\"] = estreams_attributes[\"baseflow_index\"]\n",
    "geology_regional_31_classes_moselle.corr(method=\"pearson\")\n",
    "\n",
    "# Concatenation\n",
    "estreams_attributes[[\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]] = geology_regional_31_classes_moselle[[\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]]\n",
    "\n",
    "# Adjust the three categories for also global dataset\n",
    "estreams_attributes[\"perm_high_glob2\"] = estreams_attributes[\"perm_high_glob\"]\n",
    "estreams_attributes[\"perm_medium_glob2\"] = estreams_attributes[\"perm_medium_glob\"] + estreams_attributes[\"perm_low_glob\"]\n",
    "estreams_attributes[\"perm_low_glob2\"] = estreams_attributes[\"perm_verylow_glob\"]\n",
    "\n",
    "###########################################################################################################################\n",
    "# Adjust the columns of the dataset:\n",
    "for basin_id in estreams_attributes.index.tolist():\n",
    "\n",
    "    # Extract and divide by 100\n",
    "    v1 = estreams_attributes.loc[basin_id, \"perm_high_regi\"] / 100\n",
    "    v2 = estreams_attributes.loc[basin_id, \"perm_medium_regi\"] / 100\n",
    "    v3 = estreams_attributes.loc[basin_id, \"perm_low_regi\"] / 100\n",
    "\n",
    "    # Round all values to one decimal place\n",
    "    v1 = round(v1, 2)\n",
    "    v2 = round(v2, 2)\n",
    "    v3 = round(v3, 2)\n",
    "\n",
    "    # Ensure the sum is exactly 1 by adjusting the largest value\n",
    "    diff = 1 - (v1 + v2 + v3)\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the value that was the largest before rounding\n",
    "        if max(v1, v2, v3) == v1:\n",
    "            v1 += diff\n",
    "        elif max(v1, v2, v3) == v2:\n",
    "            v2 += diff\n",
    "        else:\n",
    "            v3 += diff\n",
    "\n",
    "    # Assign back\n",
    "    estreams_attributes.loc[basin_id, \"perm_high_regi\"] = v1 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_medium_regi\"] = v2 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_low_regi\"] = v3 * 100\n",
    "\n",
    "\n",
    "for basin_id in estreams_attributes.index.tolist():\n",
    "\n",
    "    # Extract and divide by 100\n",
    "    v1 = estreams_attributes.loc[basin_id, \"perm_high_glob2\"] / 100\n",
    "    v2 = estreams_attributes.loc[basin_id, \"perm_medium_glob2\"] / 100\n",
    "    v3 = estreams_attributes.loc[basin_id, \"perm_low_glob2\"] / 100\n",
    "\n",
    "    # Round all values to one decimal place\n",
    "    v1 = round(v1, 2)\n",
    "    v2 = round(v2, 2)\n",
    "    v3 = round(v3, 2)\n",
    "\n",
    "    # Ensure the sum is exactly 1 by adjusting the largest value\n",
    "    diff = 1 - (v1 + v2 + v3)\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the value that was the largest before rounding\n",
    "        if max(v1, v2, v3) == v1:\n",
    "            v1 += diff\n",
    "        elif max(v1, v2, v3) == v2:\n",
    "            v2 += diff\n",
    "        else:\n",
    "            v3 += diff\n",
    "\n",
    "    # Assign back\n",
    "    estreams_attributes.loc[basin_id, \"perm_high_glob2\"] = v1 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_medium_glob2\"] = v2 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_low_glob2\"] = v3 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c1bc546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions\n",
    "def obj_fun_nsee(observations, simulation, expo=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Squared Error Efficiency (NSEE) while ensuring that\n",
    "    NaNs in simulation are NOT masked (only NaNs in observations are masked).\n",
    "\n",
    "    Parameters:\n",
    "        observations (array-like): Observed values (with fixed NaNs).\n",
    "        simulation (array-like): Simulated values (can contain NaNs).\n",
    "        expo (float, optional): Exponent applied to observations and simulations. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        float: NSEE score (higher values indicate worse performance).\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    simulation = np.asarray(simulation)\n",
    "\n",
    "    # Mask only NaNs in observations\n",
    "    mask = ~np.isnan(observations)\n",
    "    obs = observations[mask]\n",
    "    sim = simulation[mask]  # Keep all simulated values, even NaNs\n",
    "\n",
    "    # If simulation contains NaNs after masking observations, return penalty\n",
    "    if np.isnan(sim).any():\n",
    "        return 10.0  # Large penalty if NaNs appear in the simulation\n",
    "\n",
    "    metric = np.sum((sim**expo - obs**expo)**2) / np.sum((obs**expo - np.mean(obs**expo))**2)\n",
    "    \n",
    "    return float(metric)\n",
    "\n",
    "\n",
    "def obj_fun_kge(observations, simulation):\n",
    "    \"\"\"\n",
    "    Calculate the KGE-2012 objective function, ensuring that NaNs in simulation are NOT masked.\n",
    "    \n",
    "    Parameters:\n",
    "        observations (array-like): Observed values (with fixed NaNs).\n",
    "        simulation (array-like): Simulated values (can contain NaNs).\n",
    "\n",
    "    Returns:\n",
    "        float: KGE-2012 score (higher values indicate worse performance).\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    simulation = np.asarray(simulation)\n",
    "\n",
    "    # Mask only NaNs in observations\n",
    "    mask = ~np.isnan(observations)\n",
    "    obs = observations[mask]\n",
    "    sim = simulation[mask]  # Keep all simulated values, even NaNs\n",
    "\n",
    "    # Check if there are NaNs in the simulation after masking obs\n",
    "    if np.isnan(sim).any():\n",
    "        return 10.0  # Large penalty if the simulation contains NaNs\n",
    "    \n",
    "    obs_mean = np.mean(obs)\n",
    "    sim_mean = np.mean(sim)\n",
    "\n",
    "    r = np.corrcoef(obs, sim)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs)\n",
    "    beta = sim_mean / obs_mean\n",
    "\n",
    "    kge = np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)  # KGE-2012\n",
    "\n",
    "    return float(kge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d064d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 548.56it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 813.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# First we define the outlet of the Moselle to be used\n",
    "outlets = [\"DEBU1959\"]\n",
    "nested_cats_df = nested_catchments.loc[outlets, :]\n",
    "\n",
    "# Now we save our dataframes in a dictionary of dataframes. One dataframe for each watershed. \n",
    "\n",
    "nested_cats_filtered = find_max_unique_rows(nested_cats_df)                                  # Filter only the catchemnts using the function stated before\n",
    "nested_cats_filtered_df = nested_catchments.loc[nested_cats_filtered, :]                     # Here we filter the catchemnts for the list (again, after we apply our function):\n",
    "\n",
    "# Store the variables for the selected catchments in a list of dataframes now for only the ones above 20 cats:\n",
    "estreams_attributes_dfs = {}\n",
    "for catchment in tqdm.tqdm(nested_cats_filtered):\n",
    "    # Retrieve the nested list of catchments for the current catchment\n",
    "    nested_clip = nested_cats_filtered_df.loc[catchment, 'nested_catchments']\n",
    "    \n",
    "    # Filter values to include only those that exist in the index of estreams_attributes\n",
    "    nested_clip = [value for value in nested_clip if value in estreams_attributes.index]\n",
    "    \n",
    "    # Filter the estreams_attributes DataFrame based on the filtered nested_clip\n",
    "    cat_clip = estreams_attributes.loc[nested_clip, :]\n",
    "    \n",
    "    # Store the resulting DataFrame in the dictionary\n",
    "    estreams_attributes_dfs[catchment] = cat_clip\n",
    "\n",
    "# Here we can save the length of each watershed (number of nested catchemnts)\n",
    "catchment_lens = pd.DataFrame(index = estreams_attributes_dfs.keys())\n",
    "for catchment, data in estreams_attributes_dfs.items():\n",
    "    catchment_lens.loc[catchment, \"len\"] = len(data)\n",
    "\n",
    "# Now we can filter it properly:\n",
    "nested_cats_filtered_abovevalue = catchment_lens[catchment_lens.len >= 10]\n",
    "\n",
    "# # Here we filter the catchemnts for the list (again, after we apply our function):\n",
    "nested_cats_filtered_abovevalue_df = nested_catchments.loc[nested_cats_filtered_abovevalue.index, :]\n",
    "\n",
    "# Store the variables for the selected catchments in a list of dataframes now for only the ones above 20 cats:\n",
    "estreams_attributes_dfs = {}\n",
    "\n",
    "for catchment in tqdm.tqdm(nested_cats_filtered_abovevalue_df.index):\n",
    "    # Retrieve the nested list of catchments for the current catchment\n",
    "    nested_clip = nested_cats_filtered_abovevalue_df.loc[catchment, 'nested_catchments']\n",
    "    \n",
    "    # Filter values to include only those that exist in the index of estreams_attributes\n",
    "    nested_clip = [value for value in nested_clip if value in estreams_attributes.index]\n",
    "    \n",
    "    # Filter the estreams_attributes DataFrame based on the filtered nested_clip\n",
    "    cat_clip = estreams_attributes.loc[nested_clip, :]\n",
    "    \n",
    "    # Store the resulting DataFrame in the dictionary\n",
    "    estreams_attributes_dfs[catchment] = cat_clip\n",
    "\n",
    "# Adjust and clip it:\n",
    "estreams_attributes_clipped = estreams_attributes_dfs[\"DEBU1959\"]\n",
    "\n",
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "estreams_attributes_clipped['start_date'] = pd.to_datetime(estreams_attributes_clipped['start_date'])\n",
    "estreams_attributes_clipped['end_date'] = pd.to_datetime(estreams_attributes_clipped['end_date'])\n",
    "\n",
    "\n",
    "#estreams_attributes_clipped_filters = estreams_attributes_clipped[estreams_attributes_clipped.end_date >= \"2010\"]\n",
    "#estreams_attributes_clipped_filters = estreams_attributes_clipped_filters[estreams_attributes_clipped_filters.start_date <= \"2002\"]\n",
    "\n",
    "# Here we retrieve the conectivity (from EStreams computation)\n",
    "# Load the nested catchments CSV file\n",
    "df = pd.read_excel(\"../data/nested_catchments.xlsx\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={df.columns[1]: \"basin_id\", df.columns[2]: \"connected_basin_id\"})\n",
    "df = df.drop(columns=[df.columns[0]])  # Drop the unnamed index column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72afebd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>gauge_id</th>\n",
       "      <th>gauge_name</th>\n",
       "      <th>gauge_country</th>\n",
       "      <th>gauge_provider</th>\n",
       "      <th>river</th>\n",
       "      <th>lon_snap</th>\n",
       "      <th>lat_snap</th>\n",
       "      <th>lon</th>\n",
       "      <th>lat</th>\n",
       "      <th>...</th>\n",
       "      <th>irri_1990</th>\n",
       "      <th>irri_2005</th>\n",
       "      <th>stations_num_p_mean</th>\n",
       "      <th>perm_high_regi</th>\n",
       "      <th>perm_medium_regi</th>\n",
       "      <th>perm_low_regi</th>\n",
       "      <th>perm_high_glob2</th>\n",
       "      <th>perm_medium_glob2</th>\n",
       "      <th>perm_low_glob2</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>basin_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LU000018</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>Schoenfels</td>\n",
       "      <td>LU</td>\n",
       "      <td>LU_CONTACTFORM</td>\n",
       "      <td>Mamer</td>\n",
       "      <td>6.100795</td>\n",
       "      <td>49.723112</td>\n",
       "      <td>6.100795</td>\n",
       "      <td>49.723112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>17.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000010</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>Hunnebuer</td>\n",
       "      <td>LU</td>\n",
       "      <td>LU_CONTACTFORM</td>\n",
       "      <td>Eisch</td>\n",
       "      <td>6.079524</td>\n",
       "      <td>49.729184</td>\n",
       "      <td>6.079524</td>\n",
       "      <td>49.729184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>16.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LU000001</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>Bigonville</td>\n",
       "      <td>LU</td>\n",
       "      <td>LU_CONTACTFORM</td>\n",
       "      <td>Sure</td>\n",
       "      <td>5.801399</td>\n",
       "      <td>49.869821</td>\n",
       "      <td>5.801399</td>\n",
       "      <td>49.869821</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERP2028</th>\n",
       "      <td>3</td>\n",
       "      <td>2674030900</td>\n",
       "      <td>Eisenschmitt</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE_RP</td>\n",
       "      <td>Salm</td>\n",
       "      <td>6.718000</td>\n",
       "      <td>50.048000</td>\n",
       "      <td>6.718000</td>\n",
       "      <td>50.048000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR000183</th>\n",
       "      <td>4</td>\n",
       "      <td>A900105050</td>\n",
       "      <td>A9001050</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Sarre à Laneuveville-lès-Lorquin</td>\n",
       "      <td>7.008689</td>\n",
       "      <td>48.654579</td>\n",
       "      <td>7.008689</td>\n",
       "      <td>48.654579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Group_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR003271</th>\n",
       "      <td>107</td>\n",
       "      <td>A782101001</td>\n",
       "      <td>La Seille Ã  Nomeny</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Seille à Nomeny</td>\n",
       "      <td>6.227788</td>\n",
       "      <td>48.888271</td>\n",
       "      <td>6.227788</td>\n",
       "      <td>48.888271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429</td>\n",
       "      <td>0.436</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR003301</th>\n",
       "      <td>108</td>\n",
       "      <td>A930108040</td>\n",
       "      <td>La Sarre Ã  Wittring</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Sarre à Wittring</td>\n",
       "      <td>7.150066</td>\n",
       "      <td>49.053225</td>\n",
       "      <td>7.150066</td>\n",
       "      <td>49.053225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436</td>\n",
       "      <td>2.205</td>\n",
       "      <td>16.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DERP2003</th>\n",
       "      <td>109</td>\n",
       "      <td>2620050500</td>\n",
       "      <td>Bollendorf</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE_RP</td>\n",
       "      <td>Sauer</td>\n",
       "      <td>6.359000</td>\n",
       "      <td>49.851000</td>\n",
       "      <td>6.359000</td>\n",
       "      <td>49.851000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.627</td>\n",
       "      <td>4.160</td>\n",
       "      <td>65.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEBU1958</th>\n",
       "      <td>110</td>\n",
       "      <td>26500100</td>\n",
       "      <td>BundespegelTrierUp</td>\n",
       "      <td>DE</td>\n",
       "      <td>DE_BU</td>\n",
       "      <td>Mosel</td>\n",
       "      <td>6.627000</td>\n",
       "      <td>49.732000</td>\n",
       "      <td>6.627000</td>\n",
       "      <td>49.732000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.862</td>\n",
       "      <td>12.761</td>\n",
       "      <td>219.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FR000184</th>\n",
       "      <td>111</td>\n",
       "      <td>A901305050</td>\n",
       "      <td>A9013050</td>\n",
       "      <td>FR</td>\n",
       "      <td>FR_EAUFRANCE</td>\n",
       "      <td>La Sarre Rouge à Vasperviller</td>\n",
       "      <td>7.060836</td>\n",
       "      <td>48.640785</td>\n",
       "      <td>7.064290</td>\n",
       "      <td>48.640085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Group_7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 127 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Unnamed: 0    gauge_id            gauge_name gauge_country  \\\n",
       "basin_id                                                               \n",
       "LU000018           0           5            Schoenfels            LU   \n",
       "LU000010           1           6             Hunnebuer            LU   \n",
       "LU000001           2          17            Bigonville            LU   \n",
       "DERP2028           3  2674030900          Eisenschmitt            DE   \n",
       "FR000183           4  A900105050              A9001050            FR   \n",
       "...              ...         ...                   ...           ...   \n",
       "FR003271         107  A782101001   La Seille Ã  Nomeny            FR   \n",
       "FR003301         108  A930108040  La Sarre Ã  Wittring            FR   \n",
       "DERP2003         109  2620050500            Bollendorf            DE   \n",
       "DEBU1958         110    26500100    BundespegelTrierUp            DE   \n",
       "FR000184         111  A901305050              A9013050            FR   \n",
       "\n",
       "          gauge_provider                                river  lon_snap  \\\n",
       "basin_id                                                                  \n",
       "LU000018  LU_CONTACTFORM                                Mamer  6.100795   \n",
       "LU000010  LU_CONTACTFORM                                Eisch  6.079524   \n",
       "LU000001  LU_CONTACTFORM                                 Sure  5.801399   \n",
       "DERP2028           DE_RP                                 Salm  6.718000   \n",
       "FR000183    FR_EAUFRANCE  La Sarre à Laneuveville-lès-Lorquin  7.008689   \n",
       "...                  ...                                  ...       ...   \n",
       "FR003271    FR_EAUFRANCE                   La Seille à Nomeny  6.227788   \n",
       "FR003301    FR_EAUFRANCE                  La Sarre à Wittring  7.150066   \n",
       "DERP2003           DE_RP                                Sauer  6.359000   \n",
       "DEBU1958           DE_BU                                Mosel  6.627000   \n",
       "FR000184    FR_EAUFRANCE        La Sarre Rouge à Vasperviller  7.060836   \n",
       "\n",
       "           lat_snap       lon        lat  ...  irri_1990  irri_2005  \\\n",
       "basin_id                                  ...                         \n",
       "LU000018  49.723112  6.100795  49.723112  ...      0.015      0.015   \n",
       "LU000010  49.729184  6.079524  49.729184  ...      0.026      0.026   \n",
       "LU000001  49.869821  5.801399  49.869821  ...      0.000      0.000   \n",
       "DERP2028  50.048000  6.718000  50.048000  ...      0.000      0.000   \n",
       "FR000183  48.654579  7.008689  48.654579  ...      0.000      0.000   \n",
       "...             ...       ...        ...  ...        ...        ...   \n",
       "FR003271  48.888271  6.227788  48.888271  ...      0.429      0.436   \n",
       "FR003301  49.053225  7.150066  49.053225  ...      0.436      2.205   \n",
       "DERP2003  49.851000  6.359000  49.851000  ...      1.627      4.160   \n",
       "DEBU1958  49.732000  6.627000  49.732000  ...     10.862     12.761   \n",
       "FR000184  48.640785  7.064290  48.640085  ...      0.000      0.000   \n",
       "\n",
       "          stations_num_p_mean  perm_high_regi  perm_medium_regi perm_low_regi  \\\n",
       "basin_id                                                                        \n",
       "LU000018                 17.0            39.0              61.0           0.0   \n",
       "LU000010                 16.0            42.0              58.0           0.0   \n",
       "LU000001                  9.0             1.0               0.0          99.0   \n",
       "DERP2028                 10.0            80.0               7.0          13.0   \n",
       "FR000183                  4.0            66.0              29.0           5.0   \n",
       "...                       ...             ...               ...           ...   \n",
       "FR003271                  5.0             8.0              92.0           0.0   \n",
       "FR003301                 16.0            17.0              83.0           0.0   \n",
       "DERP2003                 65.0            17.0              30.0          53.0   \n",
       "DEBU1958                219.0            25.0              60.0          15.0   \n",
       "FR000184                  5.0            98.0               2.0           0.0   \n",
       "\n",
       "         perm_high_glob2  perm_medium_glob2  perm_low_glob2    group  \n",
       "basin_id                                                              \n",
       "LU000018             0.0              100.0             0.0  Group_1  \n",
       "LU000010             1.0               99.0             0.0  Group_1  \n",
       "LU000001           100.0                0.0             0.0  Group_1  \n",
       "DERP2028            79.0               20.0             1.0  Group_1  \n",
       "FR000183            64.0               30.0             6.0  Group_1  \n",
       "...                  ...                ...             ...      ...  \n",
       "FR003271             0.0              100.0             0.0  Group_7  \n",
       "FR003301            15.0               85.0             0.0  Group_7  \n",
       "DERP2003            50.0               50.0             0.0  Group_7  \n",
       "DEBU1958            28.0               68.0             4.0  Group_7  \n",
       "FR000184            96.0                4.0             0.0  Group_7  \n",
       "\n",
       "[112 rows x 127 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset network\n",
    "estreams_attributes_clipped_filters = pd.read_csv(R'../data/network_estreams_moselle_108_gauges.csv', encoding='utf-8')\n",
    "estreams_attributes_clipped_filters.set_index(\"basin_id\", inplace = True)\n",
    "estreams_attributes_clipped_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5bfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python implementation\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.framework.node import Node\n",
    "from superflexpy.framework.network import Network\n",
    "\n",
    "from superflexpy.implementation.elements.hbv import UnsaturatedReservoir, PowerReservoir\n",
    "\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerPython\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusPython\n",
    "\n",
    "# Numba implementation:\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusNumba\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerNumba\n",
    "\n",
    "from superflexpy.implementation.elements.hbv import PowerReservoir\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.implementation.elements.thur_model_hess import SnowReservoir, UnsaturatedReservoir, PowerReservoir, HalfTriangularLag\n",
    "\n",
    "from superflexpy.implementation.elements.structure_elements import Transparent, Junction, Splitter\n",
    "from superflexpy.framework.element import ParameterizedElement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52343368",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_finder = PegasusNumba()\n",
    "num_app = ImplicitEulerNumba(root_finder=root_finder)\n",
    "\n",
    "class ParameterizedSingleFluxSplitter(ParameterizedElement):\n",
    "    _num_downstream = 2\n",
    "    _num_upstream = 1\n",
    "    \n",
    "    def set_input(self, input):\n",
    "\n",
    "        self.input = {'Q_in': input[0]}\n",
    "\n",
    "    def get_output(self, solve=True):\n",
    "\n",
    "        split_par = self._parameters[self._prefix_parameters + 'splitpar']\n",
    "\n",
    "        output1 = [self.input['Q_in'] * split_par]\n",
    "        output2 = [self.input['Q_in'] * (1 - split_par)]\n",
    "        \n",
    "        return [output1, output2]   \n",
    "    \n",
    "    \n",
    "lower_splitter = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.5},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "lower_splitter_medium = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.6},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "lower_splitter_high = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.7},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "# Fluxes in the order P, T, PET\n",
    "upper_splitter = Splitter(\n",
    "    direction=[\n",
    "        [0, 1, None],    # P and T go to the snow reservoir\n",
    "        [2, None, None]  # PET goes to the transparent element\n",
    "    ],\n",
    "    weight=[\n",
    "        [1.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ],\n",
    "    id='upper-splitter'\n",
    ")\n",
    "\n",
    "snow = SnowReservoir(\n",
    "    parameters={'t0': 0.0, 'k': 0.01, 'm': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='snow'\n",
    ")\n",
    "\n",
    "upper_transparent = Transparent(\n",
    "    id='upper-transparent'\n",
    ")\n",
    "\n",
    "upper_junction = Junction(\n",
    "    direction=[\n",
    "        [0, None],\n",
    "        [None, 0]\n",
    "    ],\n",
    "    id='upper-junction'\n",
    ")\n",
    "\n",
    "\n",
    "unsaturated = UnsaturatedReservoir(\n",
    "    parameters={'Smax': 150.0, 'Ce': 1.0, 'm': 0.01, 'beta': 2.0},\n",
    "    states={'S0': 10.0},\n",
    "    approximation=num_app,\n",
    "    id='unsaturated'\n",
    ")\n",
    "\n",
    "fast = PowerReservoir(\n",
    "    parameters={'k': 0.01, 'alpha': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='fast'\n",
    ")\n",
    "\n",
    "slow = PowerReservoir(\n",
    "    parameters={'k': 1e-4, 'alpha': 1.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='slow'\n",
    ")\n",
    "\n",
    "slowhigh = PowerReservoir(\n",
    "    parameters={'k': 1e-4, 'alpha': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='slowhigh'\n",
    ")\n",
    "\n",
    "\n",
    "lower_junction = Junction(\n",
    "    direction=[\n",
    "        [0, 0]\n",
    "    ],\n",
    "    id='lower-junction'\n",
    ")\n",
    "\n",
    "lag_fun = HalfTriangularLag(\n",
    "    parameters={'lag-time': 4.0},\n",
    "    states={'lag': None},\n",
    "    id='lag-fun'\n",
    ")\n",
    "\n",
    "lower_transparent = Transparent(\n",
    "    id='lower-transparent'\n",
    ")\n",
    "\n",
    "lower_transparent2 = Transparent(\n",
    "    id='lower-transparent2'\n",
    ")\n",
    "\n",
    "general = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "    id='general'\n",
    ")\n",
    "\n",
    "low = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [fast],\n",
    "    ],\n",
    "    id='low'\n",
    ")\n",
    "\n",
    "high = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [slowhigh],\n",
    "    ],\n",
    "    id='high'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa00aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Dictionary to store all parameter dicts\n",
    "all_param_dicts = {}\n",
    "\n",
    "# Loop through all CSVs in the current directory\n",
    "for filepath in glob.glob(\"../results/groups/*moselle*comp*.csv\"):\n",
    "    file_key = os.path.splitext(os.path.basename(filepath))[0]  # Strip .csv\n",
    "    \n",
    "    param_dict = {}\n",
    "\n",
    "    # Read file and parse lines\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\",\"):  # Skip empty or malformed lines\n",
    "                continue\n",
    "            parts = line.split(\",\")\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                try:\n",
    "                    param_dict[key] = float(value)\n",
    "                except ValueError:\n",
    "                    pass  # Skip lines where value is not a float\n",
    "            else:\n",
    "                pass  # Skip malformed lines\n",
    "\n",
    "    # Store the parsed dictionary\n",
    "    all_param_dicts[file_key] = param_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5307b719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['moselle_best_params_globcomp_Group_7_2', 'moselle_best_params_globcomp_Group_5_2', 'moselle_best_params_contcomp_Group_2_2', 'moselle_best_params_globcomp_Group_1_2', 'moselle_best_params_contcomp_Group_6_2', 'moselle_best_params_contcomp_Group_4_2', 'moselle_best_params_globcomp_Group_3_2', 'moselle_best_params_globcomp_Group_4_2', 'moselle_best_params_contcomp_Group_3_2', 'moselle_best_params_contcomp_Group_1_2', 'moselle_best_params_globcomp_Group_6_2', 'moselle_best_params_contcomp_Group_5_2', 'moselle_best_params_globcomp_Group_2_2', 'moselle_best_params_contcomp_Group_7_2', 'moselle_best_params_contcomp_Group_3', 'moselle_best_params_globcomp_Group_4', 'moselle_best_params_regicomp_Group_6_2', 'moselle_best_params_globcomp_Group_5', 'moselle_best_params_contcomp_Group_2', 'moselle_best_params_regicomp_Group_4_2', 'moselle_best_params_globcomp_Group_7', 'moselle_best_params_globcomp_Group_6', 'moselle_best_params_contcomp_Group_1', 'moselle_best_params_contcomp_Group_5', 'moselle_best_params_globcomp_Group_2', 'moselle_best_params_globcomp_Group_3', 'moselle_best_params_contcomp_Group_4', 'moselle_best_params_contcomp_Group_6', 'moselle_best_params_regicomp_Group_2_2', 'moselle_best_params_globcomp_Group_1', 'moselle_best_params_contcomp_Group_7', 'moselle_best_params_regicomp_Group_5_2', 'moselle_best_params_regicomp_Group_3', 'moselle_best_params_regicomp_Group_2', 'moselle_best_params_regicomp_Group_1', 'moselle_best_params_regicomp_Group_7_2', 'moselle_best_params_regicomp_Group_3_2', 'moselle_best_params_regicomp_Group_5', 'moselle_best_params_regicomp_Group_4', 'moselle_best_params_regicomp_Group_6', 'moselle_best_params_regicomp_Group_1_2', 'moselle_best_params_regicomp_Group_7'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_param_dicts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "594a992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments_ids = estreams_attributes_clipped_filters.index.tolist()\n",
    "\n",
    "def calculate_hydro_year(date, first_month=10):\n",
    "    \"\"\"\n",
    "    This function calculates the hydrological year from a date. The\n",
    "    hydrological year starts on the month defined by the parameter first_month.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : pandas.core.indexes.datetimes.DatetimeIndex\n",
    "        Date series\n",
    "    first_month : int\n",
    "        Number of the first month of the hydrological year\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Hydrological year time series\n",
    "    \"\"\"\n",
    "\n",
    "    hydrological_year = date.year.values.copy()\n",
    "    hydrological_year[date.month >= first_month] += 1\n",
    "\n",
    "    return hydrological_year\n",
    "\n",
    "def run_model_superflexpy(catchments_ids, best_params_dict_model, perm_areas_model):\n",
    "    # Run the iterative function\n",
    "    iterative_immediate_downstream = find_iterative_immediate_downstream(df, catchments_ids)\n",
    "\n",
    "    # Convert results to a DataFrame for display\n",
    "    iterative_downstream_df = pd.DataFrame(iterative_immediate_downstream.items(), \n",
    "                                        columns=['basin_id', 'immediate_downstream_basin'])\n",
    "\n",
    "\n",
    "    # Assuming the DataFrame has columns 'basin_id' and 'downstream_id'\n",
    "    topology_list = {basin: None for basin in catchments_ids}  # Default to None\n",
    "\n",
    "    # Filter DataFrame for relevant basin_ids and update topology\n",
    "    for _, row in iterative_downstream_df.iterrows():\n",
    "        if row['basin_id'] in topology_list:\n",
    "            topology_list[row['basin_id']] = row['immediate_downstream_basin']\n",
    "\n",
    "    # Generate Nodes dynamically and assign them as global variables\n",
    "    catchments = [] # Dictionary to store nodes\n",
    "    \n",
    "    general = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "    id='general')\n",
    "\n",
    "    low = Unit(\n",
    "        layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "        id='low')\n",
    "\n",
    "    high = Unit(\n",
    "        layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "        id='high')\n",
    "\n",
    "    for cat_id in catchments_ids:\n",
    "        node = Node(\n",
    "            units=[high, general, low],  # Use unit from dictionary or default\n",
    "            weights=perm_areas_model[cat_id],\n",
    "            area=areas.get(cat_id),  # Use predefined area or default\n",
    "            id=cat_id\n",
    "        )\n",
    "        catchments.append(node)  # Store in the list\n",
    "\n",
    "        # Assign the node as a global variable\n",
    "        globals()[cat_id] = node\n",
    "\n",
    "\n",
    "    # Ensure topology only includes nodes that exist in `catchments_ids`\n",
    "    topology = {\n",
    "        cat_id: upstream if upstream in catchments_ids else None\n",
    "        for cat_id, upstream in topology_list.items() if cat_id in catchments_ids\n",
    "    }\n",
    "\n",
    "    # Create the Network\n",
    "    model = Network(\n",
    "        nodes=catchments,  # Pass list of Node objects\n",
    "        topology=topology  \n",
    "    )\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    # Set inputs for each node using the manually defined dictionary\n",
    "    for cat in catchments:\n",
    "        cat.set_input(inputs[cat.id])  # Correct way to set inputs\n",
    "\n",
    "    model.set_timestep(1.0)\n",
    "    model.set_parameters(best_params_dict_model)\n",
    "\n",
    "    output = model.get_output()\n",
    "\n",
    "    return output\n",
    "\n",
    "def is_valid_key(k):\n",
    "    # Exclude keys that end in Group_X_2\n",
    "    return not re.search(r'Group_\\d+_2$', k)\n",
    "\n",
    "def is_valid_key_2(k):\n",
    "    # Include only keys that end in Group_X_2\n",
    "    return re.search(r'Group_\\d+_2$', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a700755",
   "metadata": {},
   "source": [
    "## Model all time-series using all possible combinations of params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b421b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04, 0.0, 0.96]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4d65c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:26<00:00,  3.74s/it]\n",
      "100%|██████████| 7/7 [00:25<00:00,  3.59s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "catchments_ids = [\"FR003250\", \"FR000185\", \"FR003259\"]\n",
    "perm_areas[\"FR003250\"] = [0.96, 0.0, 0.04]\n",
    "perm_areas[\"FR000185\"] = [0.02, 0.23, 0.75]\n",
    "perm_areas[\"FR003259\"] = [0.00, 0.34, 0.66]\n",
    "\n",
    "# Filter keys\n",
    "regional_keys = [k for k in all_param_dicts if \"regi\" in k and is_valid_key(k)]\n",
    "continental_keys = [k for k in all_param_dicts if \"cont\" in k and is_valid_key(k)]\n",
    "global_keys = [k for k in all_param_dicts if \"glob\" in k and is_valid_key(k)]\n",
    "\n",
    "output_regional_dict = {}\n",
    "output_continental_dict = {}\n",
    "output_global_dict = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_dict[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_dict[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c33a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:24<00:00,  3.52s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.46s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "catchments_ids = [\"FR003250\", \"FR000185\", \"FR003259\"]\n",
    "perm_areas[\"FR003250\"] = [0.96, 0.0, 0.04]\n",
    "perm_areas[\"FR000185\"] = [0.02, 0.23, 0.75]\n",
    "perm_areas[\"FR003259\"] = [0.00, 0.34, 0.66]\n",
    "\n",
    "# Filter keys\n",
    "regional_keys_2 = [k for k in all_param_dicts if \"regi\" in k and is_valid_key_2(k)]\n",
    "continental_keys_2 = [k for k in all_param_dicts if \"cont\" in k and is_valid_key_2(k)]\n",
    "global_keys_2 = [k for k in all_param_dicts if \"glob\" in k and is_valid_key_2(k)]\n",
    "\n",
    "output_regional_dict_8801 = {}\n",
    "output_continental_dict_8801 = {}\n",
    "output_global_dict_8801 = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict_8801[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_dict_8801[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_dict_8801[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a35136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the concatenated data for the complete series analysis\n",
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "observations1 = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "quality_masks1 = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "\n",
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "observations2 = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "quality_masks2 = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "\n",
    "observations_cal = {}\n",
    "\n",
    "for key in observations1.keys():\n",
    "    arr1 = np.atleast_1d(observations1[key])\n",
    "    arr2 = np.atleast_1d(observations2.get(key, np.array([])))\n",
    "\n",
    "    # Always concatenate arrays, even if they contain NaNs or are empty\n",
    "    #observations_cal[key] = np.concatenate([arr1, arr2])\n",
    "    # Remove the first 365 days from the second dataset\n",
    "    arr2_trimmed = arr2[365:] if arr2.size > 365 else np.array([])\n",
    "\n",
    "    observations_cal[key] = np.concatenate([arr1, arr2_trimmed])\n",
    "\n",
    "quality_masks_cal = {}\n",
    "\n",
    "for key in quality_masks1.keys():\n",
    "    arr1 = np.atleast_1d(quality_masks1[key])\n",
    "    arr2 = np.atleast_1d(quality_masks2.get(key, np.array([])))\n",
    "\n",
    "    # Always concatenate arrays, even if they contain NaNs or are empty\n",
    "    #quality_masks_cal[key] = np.concatenate([arr1, arr2])\n",
    "    # Remove the first 365 days from the second dataset\n",
    "    arr2_trimmed = arr2[365:] if arr2.size > 365 else np.array([])\n",
    "\n",
    "    quality_masks_cal[key] = np.concatenate([arr1, arr2_trimmed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d96c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load both input files\n",
    "path_inputs_1 = '../data/models/input/subset_1988_2001/inputs.npy'\n",
    "path_inputs_2 = '../data/models/input/subset_2001_2015/inputs.npy'\n",
    "\n",
    "inputs1 = np.load(path_inputs_1, allow_pickle=True).item()\n",
    "inputs2 = np.load(path_inputs_2, allow_pickle=True).item()\n",
    "\n",
    "# Initialize new dictionaries\n",
    "precipitation_cal = {}\n",
    "temperature_cal = {}\n",
    "evaporation_cal = {}\n",
    "\n",
    "for key in inputs1.keys():\n",
    "    # Get (P, T, PET) tuples from each period\n",
    "    p1, t1, pet1 = map(np.atleast_1d, inputs1[key])\n",
    "    p2, t2, pet2 = map(np.atleast_1d, inputs2.get(key, ([], [], [])))\n",
    "\n",
    "\n",
    "    p2 = p2[365:] if p2.size > 365 else np.array([])\n",
    "    t2 = t2[365:] if t2.size > 365 else np.array([])\n",
    "    pet2 = pet2[365:] if pet2.size > 365 else np.array([])\n",
    "\n",
    "\n",
    "    # Concatenate and assign\n",
    "    precipitation_cal[key] = np.concatenate([p1, p2])\n",
    "    temperature_cal[key] = np.concatenate([t1, t2])\n",
    "    evaporation_cal[key] = np.concatenate([pet1, pet2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "153e77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_global_dict_cal = {}\n",
    "\n",
    "for param_key in output_global_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_global_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_global_dict[param_key]:\n",
    "            if gauge_id in output_global_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_global_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_global_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_8801, series_recent])\n",
    "                # Remove first 365 days from recent series\n",
    "                series_recent_trimmed = series_recent[365:] if series_recent.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_8801, series_recent_trimmed])\n",
    "                \n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_global_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fec7d6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_continental_dict_cal = {}\n",
    "\n",
    "for param_key in output_continental_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_continental_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_continental_dict[param_key]:\n",
    "            if gauge_id in output_continental_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_continental_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_continental_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_8801, series_recent])\n",
    "                \n",
    "                # Remove first 365 days from recent series\n",
    "                series_recent_trimmed = series_recent[365:] if series_recent.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_8801, series_recent_trimmed])\n",
    "\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_continental_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f96314",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_regional_dict_cal = {}\n",
    "\n",
    "for param_key in output_regional_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_regional_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_regional_dict[param_key]:\n",
    "            if gauge_id in output_regional_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_regional_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_regional_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_8801, series_recent])\n",
    "                \n",
    "                # Remove first 365 days from recent series\n",
    "                series_recent_trimmed = series_recent[365:] if series_recent.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_8801, series_recent_trimmed])\n",
    "\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_regional_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21da17",
   "metadata": {},
   "source": [
    "## Space-time validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4be11a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:24<00:00,  3.50s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.47s/it]\n",
      "100%|██████████| 7/7 [00:25<00:00,  3.60s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "catchments_ids = [\"FR003250\", \"FR000185\", \"FR003259\"]\n",
    "perm_areas[\"FR003250\"] = [0.96, 0.0, 0.04]\n",
    "perm_areas[\"FR000185\"] = [0.02, 0.23, 0.75]\n",
    "perm_areas[\"FR003259\"] = [0.00, 0.34, 0.66]\n",
    "\n",
    "# Filter keys\n",
    "regional_keys = [k for k in all_param_dicts if \"regi\" in k and is_valid_key(k)]\n",
    "continental_keys = [k for k in all_param_dicts if \"cont\" in k and is_valid_key(k)]\n",
    "global_keys = [k for k in all_param_dicts if \"glob\" in k and is_valid_key(k)]\n",
    "\n",
    "output_regional_val_dict = {}\n",
    "output_continental_val_dict = {}\n",
    "output_global_val_dict = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_val_dict[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_val_dict[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_val_dict[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e47dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:25<00:00,  3.63s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.52s/it]\n",
      "100%|██████████| 7/7 [00:24<00:00,  3.50s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "catchments_ids = [\"FR003250\", \"FR000185\", \"FR003259\"]\n",
    "perm_areas[\"FR003250\"] = [0.96, 0.0, 0.04]\n",
    "perm_areas[\"FR000185\"] = [0.02, 0.23, 0.75]\n",
    "perm_areas[\"FR003259\"] = [0.00, 0.34, 0.66]\n",
    "\n",
    "# Filter keys\n",
    "regional_keys_2 = [k for k in all_param_dicts if \"regi\" in k and is_valid_key_2(k)]\n",
    "continental_keys_2 = [k for k in all_param_dicts if \"cont\" in k and is_valid_key_2(k)]\n",
    "global_keys_2 = [k for k in all_param_dicts if \"glob\" in k and is_valid_key_2(k)]\n",
    "\n",
    "output_regional_dict_0115 = {}\n",
    "output_continental_dict_0115 = {}\n",
    "output_global_dict_0115 = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict_0115[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(continental_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areascontinental\n",
    "    )\n",
    "    output_continental_dict_0115[key] = output\n",
    "\n",
    "for key in tqdm.tqdm(global_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areasglobal\n",
    "    )\n",
    "    output_global_dict_0115[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13cfe270",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_global_dict_val = {}\n",
    "\n",
    "for param_key in output_global_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_global_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_global_val_dict[param_key]:\n",
    "            if gauge_id in output_global_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_global_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_global_val_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_recent, series_8801])\n",
    "                \n",
    "                # Remove first 365 days from recent series\n",
    "                series_8801_trimmed = series_8801[365:] if series_8801.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_recent, series_8801_trimmed])\n",
    "\n",
    "                \n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_global_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "179f1353",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_continental_dict_val = {}\n",
    "\n",
    "for param_key in output_continental_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_continental_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_continental_val_dict[param_key]:\n",
    "            if gauge_id in output_continental_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_continental_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_continental_val_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_recent, series_8801])\n",
    "                \n",
    "                # Remove first 365 days from recent series\n",
    "                series_8801_trimmed = series_8801[365:] if series_8801.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_recent, series_8801_trimmed])\n",
    "\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_continental_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8705d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_regional_dict_val = {}\n",
    "\n",
    "for param_key in output_regional_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_regional_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_regional_val_dict[param_key]:\n",
    "            if gauge_id in output_regional_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_regional_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_regional_val_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_recent, series_8801])\n",
    "                \n",
    "                # Remove first 365 days from recent series\n",
    "                series_8801_trimmed = series_8801[365:] if series_8801.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_recent, series_8801_trimmed])\n",
    "\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_regional_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d195b",
   "metadata": {},
   "source": [
    "## Save the time-series in netcdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4d218e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "\n",
    "# Adjust these according to your data\n",
    "group_suffixes = [\"Group_1\", \"Group_2\", \"Group_3\", \"Group_4\", \"Group_5\", \"Group_6\", \"Group_7\"]\n",
    "\n",
    "gauge_ids = list(observations_cal.keys())  # Your observations dict should be preloaded\n",
    "time_index = pd.date_range(start=\"1988-10-01\", end=\"2015-09-30\", freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e570ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_inputs = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"observation\": ([\"gauge_id\", \"date\"], [observations_cal[g] for g in gauge_ids]),\n",
    "        \"precipitation\": ([\"gauge_id\", \"date\"], [precipitation_cal[g] for g in gauge_ids]),\n",
    "        \"temperature\": ([\"gauge_id\", \"date\"], [temperature_cal[g] for g in gauge_ids]),\n",
    "        \"evaporation\": ([\"gauge_id\", \"date\"], [evaporation_cal[g] for g in gauge_ids]),\n",
    "    },\n",
    "    coords={\n",
    "        \"gauge_id\": gauge_ids,\n",
    "        \"date\": time_index\n",
    "    }\n",
    ")\n",
    "\n",
    "ds_inputs.to_netcdf(rf\"../results/sim/FR003250_FR000185_FR003259/inputs.nc\", engine=\"scipy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31e036cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets\n",
    "datasets = {}\n",
    "\n",
    "for suffix in group_suffixes:\n",
    "    reg_key = f\"moselle_best_params_regicomp_{suffix}\"\n",
    "    cont_key = f\"moselle_best_params_contcomp_{suffix}\"\n",
    "    glob_key = f\"moselle_best_params_globcomp_{suffix}\"\n",
    "\n",
    "    reg_data = []\n",
    "    cont_data = []\n",
    "    glob_data = []\n",
    "    group_gauge_ids = []\n",
    "\n",
    "    for gauge in gauge_ids:\n",
    "        if gauge in output_regional_dict_val[reg_key] and \\\n",
    "           gauge in output_continental_dict_val[cont_key] and \\\n",
    "           gauge in output_global_dict_val[glob_key]:\n",
    "\n",
    "            reg_data.append(output_regional_dict_val[reg_key][gauge][0])\n",
    "            cont_data.append(output_continental_dict_val[cont_key][gauge][0])\n",
    "            glob_data.append(output_global_dict_val[glob_key][gauge][0])\n",
    "            group_gauge_ids.append(gauge)\n",
    "\n",
    "    if group_gauge_ids:\n",
    "        ds = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"regional\": ([\"gauge_id\", \"date\"], reg_data),\n",
    "                \"continental\": ([\"gauge_id\", \"date\"], cont_data),\n",
    "                \"global\": ([\"gauge_id\", \"date\"], glob_data)\n",
    "            },\n",
    "            coords={\n",
    "                \"gauge_id\": group_gauge_ids,\n",
    "                \"date\": time_index\n",
    "            }\n",
    "        )\n",
    "        datasets[suffix] = ds\n",
    "\n",
    "# Save each group to a separate NetCDF file using scipy (no need for netCDF4)\n",
    "for suffix, ds in datasets.items():\n",
    "    ds.to_netcdf(rf\"../results/sim/FR003250_FR000185_FR003259/simu_{suffix}.nc\", engine=\"scipy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ac82e",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0f17bc",
   "metadata": {},
   "source": [
    "# Part D: Model evaluation using no-geology \n",
    "\n",
    "## Introduction\n",
    "This notebook contains _all code_ that has been used to perform the space and space-time evaluation of the calibrated model performed for the paper \"Can more detailed geological maps improve streamflow prediction in ungauged basins?\" paper by do Nascimento et al. (in review). This part of the code covers specifically the models generated for the **no-geology** experiments. To be able to run this notebook, please ensure that you have downloaded the acompanying data of the paper. All links can be found in the data section of the paper.\n",
    "\n",
    "Author: Thiago Nascimento (thiago.nascimento@eawag.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fac3af",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "**Files**\n",
    "\n",
    "- estreams_gauging_stations.csv https://doi.org/10.5281/zenodo.14778580 (Last access: 11 February 2025)\n",
    "- estreams_geology_moselle_regional_attributes.csv https://doi.org/10.5281/zenodo.14778580 (Last access: 15 February 2025)\n",
    "- estreams_attributes_filtered_quality_geology_v01.csv https://github.com/thiagovmdon/LSH-quality_geology (Last access: 11 February 2025)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da60bb",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import spotpy\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import tqdm as tqdm\n",
    "import hydroanalysis\n",
    "from utils.functions import find_max_unique_rows\n",
    "from utils.functions import find_iterative_immediate_downstream\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.framework.node import Node\n",
    "from superflexpy.framework.network import Network\n",
    "from superflexpy.implementation.elements.hbv import UnsaturatedReservoir, PowerReservoir\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerPython\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusPython\n",
    "from superflexpy.implementation.root_finders.pegasus import PegasusNumba\n",
    "from superflexpy.implementation.numerical_approximators.implicit_euler import ImplicitEulerNumba\n",
    "from superflexpy.implementation.elements.hbv import PowerReservoir\n",
    "from superflexpy.framework.unit import Unit\n",
    "from superflexpy.implementation.elements.thur_model_hess import SnowReservoir, UnsaturatedReservoir, PowerReservoir, HalfTriangularLag\n",
    "from superflexpy.implementation.elements.structure_elements import Transparent, Junction, Splitter\n",
    "from superflexpy.framework.element import ParameterizedElement\n",
    "import re\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e88db",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0efd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where the EStreams dataset is stored\n",
    "# Eawag\n",
    "path_estreams = r'/Users/nascimth/Documents/data/EStreams'\n",
    "\n",
    "## Mac\n",
    "#path_estreams = r'/Users/thiagomedeirosdonascimento/Downloads/Python 2/Scripts/estreams_part_b/data/EStreams'\n",
    "\n",
    "path_data = r\"/Users/nascimth/Documents/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13994b",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34812cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the gauges network metadata (from EStreams):\n",
    "network_estreams = pd.read_csv(path_estreams+'/streamflow_gauges/estreams_gauging_stations.csv', encoding='utf-8')\n",
    "network_estreams.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Attributes already filtered from EStreams previously in do Nascimento et al. (2025a):\n",
    "estreams_attributes = pd.read_csv('../data/estreams_attributes_filtered_quality_geology_v01.csv', encoding='utf-8')\n",
    "estreams_attributes.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Geological attributes (regional scale). Same ones provided in do Nascimento et al. (2025a):\n",
    "geology_regional_31_classes_moselle = pd.read_csv(\"../data/estreams_geology_moselle_regional_attributes.csv\", encoding='utf-8')\n",
    "geology_regional_31_classes_moselle.set_index(\"basin_id\", inplace = True)\n",
    "\n",
    "# Here we retrieve the conectivity (from EStreams computation)\n",
    "# Load the nested catchments CSV file\n",
    "df = pd.read_excel(\"../data/nested_catchments.xlsx\")\n",
    "# Adjust the conectivity network\n",
    "# Rename columns for clarity\n",
    "df = df.rename(columns={df.columns[1]: \"basin_id\", df.columns[2]: \"connected_basin_id\"})\n",
    "df = df.drop(columns=[df.columns[0]])  # Drop the unnamed index column\n",
    "\n",
    "# Read the groups dataset\n",
    "estreams_attributes_clipped_filters = pd.read_csv(R'../data/network_estreams_moselle_108_gauges.csv', encoding='utf-8')\n",
    "estreams_attributes_clipped_filters.set_index(\"basin_id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8acc24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store all parameter dicts\n",
    "all_param_dicts = {}\n",
    "\n",
    "# Loop through all CSVs in the current directory\n",
    "for filepath in glob.glob(\"../results/groups/*moselle_best_params_nogeo*.csv\"):\n",
    "    file_key = os.path.splitext(os.path.basename(filepath))[0]  # Strip .csv\n",
    "    \n",
    "    param_dict = {}\n",
    "\n",
    "    # Read file and parse lines\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\",\"):  # Skip empty or malformed lines\n",
    "                continue\n",
    "            parts = line.split(\",\")\n",
    "            if len(parts) == 2:\n",
    "                key, value = parts\n",
    "                try:\n",
    "                    param_dict[key] = float(value)\n",
    "                except ValueError:\n",
    "                    pass  # Skip lines where value is not a float\n",
    "            else:\n",
    "                pass  # Skip malformed lines\n",
    "\n",
    "    # Store the parsed dictionary\n",
    "    all_param_dicts[file_key] = param_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6597f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fd9e5",
   "metadata": {},
   "source": [
    "### Network data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64ca750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "network_estreams['start_date'] = pd.to_datetime(network_estreams['start_date'])\n",
    "network_estreams['end_date'] = pd.to_datetime(network_estreams['end_date'])\n",
    "\n",
    "# Convert to list both the nested_catchments and the duplicated_suspect columns\n",
    "network_estreams['nested_catchments'] = network_estreams['nested_catchments'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Remove the brackets and handle NaN values\n",
    "network_estreams['duplicated_suspect'] = network_estreams['duplicated_suspect'].apply(\n",
    "    lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x)\n",
    "\n",
    "# Set the nested catchments as a dataframe\n",
    "nested_catchments = pd.DataFrame(network_estreams['nested_catchments'])\n",
    "\n",
    "# Now we add the outlet to the list (IF it was not before):\n",
    "# Ensure that the basin_id is in the nested_catchments\n",
    "for basin_id in nested_catchments.index:\n",
    "    if basin_id not in nested_catchments.at[basin_id, 'nested_catchments']:\n",
    "        nested_catchments.at[basin_id, 'nested_catchments'].append(basin_id)\n",
    "\n",
    "# Convert to list both the nested_catchments and the duplicated_suspect columns\n",
    "estreams_attributes['nested_catchments'] = estreams_attributes['nested_catchments'].apply(lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \"))\n",
    "\n",
    "# Remove the brackets and handle NaN values\n",
    "estreams_attributes['duplicated_suspect'] = estreams_attributes['duplicated_suspect'].apply(\n",
    "    lambda x: x.strip(\"[]\").replace(\"'\", \"\").split(\", \") if isinstance(x, str) else x)\n",
    "\n",
    "estreams_attributes.sort_index(inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d34fb7",
   "metadata": {},
   "source": [
    "### Geological data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5d63e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map permeability classes to corresponding columns\n",
    "permeability_columns = {\n",
    "    \"high\": [\"lit_fra_Alluvium\", 'lit_fra_Coal', 'lit_fra_Conglomerate', 'lit_fra_Gravel and sand',\n",
    "             'lit_fra_Sand', 'lit_fra_Sand and gravel', 'lit_fra_Sandstone and conglomerate', 'lit_fra_Sandstone'\n",
    "        ],\n",
    "    \n",
    "    \"medium\": ['lit_fra_Limestone', 'lit_fra_Sandstone and marl', 'lit_fra_Sandstone and schist',\n",
    "              'lit_fra_Sandstone, conglomerate and marl',\n",
    "\n",
    "              'lit_fra_Arkose', 'lit_fra_Dolomite rock', 'lit_fra_Limestone and marl', 'lit_fra_Marl', \n",
    "             'lit_fra_Marl and dolomite', 'lit_fra_Marl and limestone', 'lit_fra_Marl and sandstone',\n",
    "               'lit_fra_Sandstone and siltstone', 'lit_fra_Sandstone, siltstone and schist', \n",
    "              'lit_fra_Schist and sandstone', 'lit_fra_Silt',  'lit_fra_Silt and schist', 'lit_fra_Siltstone, sandstone and schist'\n",
    "              \n",
    "             ],\n",
    "    \n",
    "    \"low\": ['lit_fra_Cristallin basement', 'lit_fra_Plutonic rock',  'lit_fra_Quarzite',\n",
    "                    'lit_fra_Schist','lit_fra_Volcanic rock' \n",
    "                   ]\n",
    "}\n",
    "\n",
    "# Iterate over the permeability columns and calculate the area for each class\n",
    "for permeability_class, columns in permeability_columns.items():\n",
    "    geology_regional_31_classes_moselle[f'area_perm_{permeability_class}'] = geology_regional_31_classes_moselle[columns].sum(axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "geology_regional_31_classes_moselle = geology_regional_31_classes_moselle[[\"area_perm_high\", \"area_perm_medium\", \"area_perm_low\"]]\n",
    "\n",
    "# Rename the columns\n",
    "geology_regional_31_classes_moselle.columns = [\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "geology_regional_31_classes_moselle\n",
    "\n",
    "geology_regional_31_classes_moselle[\"baseflow_index\"] = estreams_attributes[\"baseflow_index\"]\n",
    "geology_regional_31_classes_moselle.corr(method=\"pearson\")\n",
    "\n",
    "# Concatenation\n",
    "estreams_attributes[[\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]] = geology_regional_31_classes_moselle[[\"perm_high_regi\", \"perm_medium_regi\", \"perm_low_regi\"]]\n",
    "\n",
    "# Adjust the three categories for also global dataset\n",
    "estreams_attributes[\"perm_high_glob2\"] = estreams_attributes[\"perm_high_glob\"]\n",
    "estreams_attributes[\"perm_medium_glob2\"] = estreams_attributes[\"perm_medium_glob\"] + estreams_attributes[\"perm_low_glob\"]\n",
    "estreams_attributes[\"perm_low_glob2\"] = estreams_attributes[\"perm_verylow_glob\"]\n",
    "\n",
    "###########################################################################################################################\n",
    "# Adjust the columns of the dataset:\n",
    "for basin_id in estreams_attributes.index.tolist():\n",
    "\n",
    "    # Extract and divide by 100\n",
    "    v1 = estreams_attributes.loc[basin_id, \"perm_high_regi\"] / 100\n",
    "    v2 = estreams_attributes.loc[basin_id, \"perm_medium_regi\"] / 100\n",
    "    v3 = estreams_attributes.loc[basin_id, \"perm_low_regi\"] / 100\n",
    "\n",
    "    # Round all values to one decimal place\n",
    "    v1 = round(v1, 2)\n",
    "    v2 = round(v2, 2)\n",
    "    v3 = round(v3, 2)\n",
    "\n",
    "    # Ensure the sum is exactly 1 by adjusting the largest value\n",
    "    diff = 1 - (v1 + v2 + v3)\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the value that was the largest before rounding\n",
    "        if max(v1, v2, v3) == v1:\n",
    "            v1 += diff\n",
    "        elif max(v1, v2, v3) == v2:\n",
    "            v2 += diff\n",
    "        else:\n",
    "            v3 += diff\n",
    "\n",
    "    # Assign back\n",
    "    estreams_attributes.loc[basin_id, \"perm_high_regi\"] = v1 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_medium_regi\"] = v2 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_low_regi\"] = v3 * 100\n",
    "\n",
    "\n",
    "for basin_id in estreams_attributes.index.tolist():\n",
    "\n",
    "    # Extract and divide by 100\n",
    "    v1 = estreams_attributes.loc[basin_id, \"perm_high_glob2\"] / 100\n",
    "    v2 = estreams_attributes.loc[basin_id, \"perm_medium_glob2\"] / 100\n",
    "    v3 = estreams_attributes.loc[basin_id, \"perm_low_glob2\"] / 100\n",
    "\n",
    "    # Round all values to one decimal place\n",
    "    v1 = round(v1, 2)\n",
    "    v2 = round(v2, 2)\n",
    "    v3 = round(v3, 2)\n",
    "\n",
    "    # Ensure the sum is exactly 1 by adjusting the largest value\n",
    "    diff = 1 - (v1 + v2 + v3)\n",
    "\n",
    "    if diff != 0:\n",
    "        # Adjust the value that was the largest before rounding\n",
    "        if max(v1, v2, v3) == v1:\n",
    "            v1 += diff\n",
    "        elif max(v1, v2, v3) == v2:\n",
    "            v2 += diff\n",
    "        else:\n",
    "            v3 += diff\n",
    "\n",
    "    # Assign back\n",
    "    estreams_attributes.loc[basin_id, \"perm_high_glob2\"] = v1 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_medium_glob2\"] = v2 * 100\n",
    "    estreams_attributes.loc[basin_id, \"perm_low_glob2\"] = v3 * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a56b1",
   "metadata": {},
   "source": [
    "## Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d064d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 460.86it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 1344.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# First we define the outlet of the Moselle to be used\n",
    "outlets = [\"DEBU1959\"]\n",
    "nested_cats_df = nested_catchments.loc[outlets, :]\n",
    "\n",
    "# Now we save our dataframes in a dictionary of dataframes. One dataframe for each watershed. \n",
    "\n",
    "nested_cats_filtered = find_max_unique_rows(nested_cats_df)                                  # Filter only the catchemnts using the function stated before\n",
    "nested_cats_filtered_df = nested_catchments.loc[nested_cats_filtered, :]                     # Here we filter the catchemnts for the list (again, after we apply our function):\n",
    "\n",
    "# Store the variables for the selected catchments in a list of dataframes now for only the ones above 20 cats:\n",
    "estreams_attributes_dfs = {}\n",
    "for catchment in tqdm.tqdm(nested_cats_filtered):\n",
    "    # Retrieve the nested list of catchments for the current catchment\n",
    "    nested_clip = nested_cats_filtered_df.loc[catchment, 'nested_catchments']\n",
    "    \n",
    "    # Filter values to include only those that exist in the index of estreams_attributes\n",
    "    nested_clip = [value for value in nested_clip if value in estreams_attributes.index]\n",
    "    \n",
    "    # Filter the estreams_attributes DataFrame based on the filtered nested_clip\n",
    "    cat_clip = estreams_attributes.loc[nested_clip, :]\n",
    "    \n",
    "    # Store the resulting DataFrame in the dictionary\n",
    "    estreams_attributes_dfs[catchment] = cat_clip\n",
    "\n",
    "# Here we can save the length of each watershed (number of nested catchemnts)\n",
    "catchment_lens = pd.DataFrame(index = estreams_attributes_dfs.keys())\n",
    "for catchment, data in estreams_attributes_dfs.items():\n",
    "    catchment_lens.loc[catchment, \"len\"] = len(data)\n",
    "\n",
    "# Now we can filter it properly:\n",
    "nested_cats_filtered_abovevalue = catchment_lens[catchment_lens.len >= 10]\n",
    "\n",
    "# # Here we filter the catchemnts for the list (again, after we apply our function):\n",
    "nested_cats_filtered_abovevalue_df = nested_catchments.loc[nested_cats_filtered_abovevalue.index, :]\n",
    "\n",
    "# Store the variables for the selected catchments in a list of dataframes now for only the ones above 20 cats:\n",
    "estreams_attributes_dfs = {}\n",
    "\n",
    "for catchment in tqdm.tqdm(nested_cats_filtered_abovevalue_df.index):\n",
    "    # Retrieve the nested list of catchments for the current catchment\n",
    "    nested_clip = nested_cats_filtered_abovevalue_df.loc[catchment, 'nested_catchments']\n",
    "    \n",
    "    # Filter values to include only those that exist in the index of estreams_attributes\n",
    "    nested_clip = [value for value in nested_clip if value in estreams_attributes.index]\n",
    "    \n",
    "    # Filter the estreams_attributes DataFrame based on the filtered nested_clip\n",
    "    cat_clip = estreams_attributes.loc[nested_clip, :]\n",
    "    \n",
    "    # Store the resulting DataFrame in the dictionary\n",
    "    estreams_attributes_dfs[catchment] = cat_clip\n",
    "\n",
    "# Adjust and clip it:\n",
    "estreams_attributes_clipped = estreams_attributes_dfs[\"DEBU1959\"]\n",
    "\n",
    "# Convert 'date_column' and 'time_column' to datetime\n",
    "estreams_attributes_clipped['start_date'] = pd.to_datetime(estreams_attributes_clipped['start_date'])\n",
    "estreams_attributes_clipped['end_date'] = pd.to_datetime(estreams_attributes_clipped['end_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72adea2",
   "metadata": {},
   "source": [
    "## Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a2c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions\n",
    "def obj_fun_nsee(observations, simulation, expo=0.5):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Squared Error Efficiency (NSEE) while ensuring that\n",
    "    NaNs in simulation are NOT masked (only NaNs in observations are masked).\n",
    "\n",
    "    Parameters:\n",
    "        observations (array-like): Observed values (with fixed NaNs).\n",
    "        simulation (array-like): Simulated values (can contain NaNs).\n",
    "        expo (float, optional): Exponent applied to observations and simulations. Default is 1.0.\n",
    "\n",
    "    Returns:\n",
    "        float: NSEE score (higher values indicate worse performance).\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    simulation = np.asarray(simulation)\n",
    "\n",
    "    # Mask only NaNs in observations\n",
    "    mask = ~np.isnan(observations)\n",
    "    obs = observations[mask]\n",
    "    sim = simulation[mask]  # Keep all simulated values, even NaNs\n",
    "\n",
    "    # If simulation contains NaNs after masking observations, return penalty\n",
    "    if np.isnan(sim).any():\n",
    "        return 10.0  # Large penalty if NaNs appear in the simulation\n",
    "\n",
    "    metric = np.sum((sim**expo - obs**expo)**2) / np.sum((obs**expo - np.mean(obs**expo))**2)\n",
    "    \n",
    "    return float(metric)\n",
    "\n",
    "\n",
    "def obj_fun_kge(observations, simulation):\n",
    "    \"\"\"\n",
    "    Calculate the KGE-2012 objective function, ensuring that NaNs in simulation are NOT masked.\n",
    "    \n",
    "    Parameters:\n",
    "        observations (array-like): Observed values (with fixed NaNs).\n",
    "        simulation (array-like): Simulated values (can contain NaNs).\n",
    "\n",
    "    Returns:\n",
    "        float: KGE-2012 score (higher values indicate worse performance).\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    simulation = np.asarray(simulation)\n",
    "\n",
    "    # Mask only NaNs in observations\n",
    "    mask = ~np.isnan(observations)\n",
    "    obs = observations[mask]\n",
    "    sim = simulation[mask]  # Keep all simulated values, even NaNs\n",
    "\n",
    "    # Check if there are NaNs in the simulation after masking obs\n",
    "    if np.isnan(sim).any():\n",
    "        return 10.0  # Large penalty if the simulation contains NaNs\n",
    "    \n",
    "    obs_mean = np.mean(obs)\n",
    "    sim_mean = np.mean(sim)\n",
    "\n",
    "    r = np.corrcoef(obs, sim)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs)\n",
    "    beta = sim_mean / obs_mean\n",
    "\n",
    "    kge = np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)  # KGE-2012\n",
    "\n",
    "    return float(kge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cd7ed7",
   "metadata": {},
   "source": [
    "## Superflexpy implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e658c6d2",
   "metadata": {},
   "source": [
    "### Defining some of the elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52343368",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_finder = PegasusNumba()\n",
    "num_app = ImplicitEulerNumba(root_finder=root_finder)\n",
    "\n",
    "# Create a spliter to be used for calibration\n",
    "class ParameterizedSingleFluxSplitter(ParameterizedElement):\n",
    "    _num_downstream = 2\n",
    "    _num_upstream = 1\n",
    "    \n",
    "    def set_input(self, input):\n",
    "\n",
    "        self.input = {'Q_in': input[0]}\n",
    "\n",
    "    def get_output(self, solve=True):\n",
    "\n",
    "        split_par = self._parameters[self._prefix_parameters + 'splitpar']\n",
    "\n",
    "        output1 = [self.input['Q_in'] * split_par]\n",
    "        output2 = [self.input['Q_in'] * (1 - split_par)]\n",
    "        \n",
    "        return [output1, output2]   \n",
    "    \n",
    "lower_splitter = ParameterizedSingleFluxSplitter(\n",
    "    parameters={'splitpar': 0.5},\n",
    "    id='lowersplitter'\n",
    ")\n",
    "\n",
    "# Fluxes in the order P, T, PET\n",
    "upper_splitter = Splitter(\n",
    "    direction=[\n",
    "        [0, 1, None],    # P and T go to the snow reservoir\n",
    "        [2, None, None]  # PET goes to the transparent element\n",
    "    ],\n",
    "    weight=[\n",
    "        [1.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]\n",
    "    ],\n",
    "    id='upper-splitter'\n",
    ")\n",
    "\n",
    "snow = SnowReservoir(\n",
    "    parameters={'t0': 0.0, 'k': 0.01, 'm': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='snow'\n",
    ")\n",
    "\n",
    "upper_transparent = Transparent(\n",
    "    id='upper-transparent'\n",
    ")\n",
    "\n",
    "upper_junction = Junction(\n",
    "    direction=[\n",
    "        [0, None],\n",
    "        [None, 0]\n",
    "    ],\n",
    "    id='upper-junction'\n",
    ")\n",
    "\n",
    "unsaturated = UnsaturatedReservoir(\n",
    "    parameters={'Smax': 150.0, 'Ce': 1.0, 'm': 0.01, 'beta': 2.0},\n",
    "    states={'S0': 10.0},\n",
    "    approximation=num_app,\n",
    "    id='unsaturated'\n",
    ")\n",
    "\n",
    "fast = PowerReservoir(\n",
    "    parameters={'k': 0.01, 'alpha': 2.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='fast'\n",
    ")\n",
    "\n",
    "slow = PowerReservoir(\n",
    "    parameters={'k': 1e-4, 'alpha': 1.0},\n",
    "    states={'S0': 0.0},\n",
    "    approximation=num_app,\n",
    "    id='slow'\n",
    ")\n",
    "\n",
    "lower_junction = Junction(\n",
    "    direction=[\n",
    "        [0, 0]\n",
    "    ],\n",
    "    id='lower-junction'\n",
    ")\n",
    "\n",
    "lag_fun = HalfTriangularLag(\n",
    "    parameters={'lag-time': 4.0},\n",
    "    states={'lag': None},\n",
    "    id='lag-fun'\n",
    ")\n",
    "\n",
    "lower_transparent = Transparent(\n",
    "    id='lower-transparent'\n",
    ")\n",
    "\n",
    "\n",
    "general = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "    id='general'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c84872",
   "metadata": {},
   "source": [
    "### Create some of the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catchments_ids = estreams_attributes_clipped_filters.index.tolist()\n",
    "\n",
    "def calculate_hydro_year(date, first_month=10):\n",
    "    \"\"\"\n",
    "    This function calculates the hydrological year from a date. The\n",
    "    hydrological year starts on the month defined by the parameter first_month.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    date : pandas.core.indexes.datetimes.DatetimeIndex\n",
    "        Date series\n",
    "    first_month : int\n",
    "        Number of the first month of the hydrological year\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Hydrological year time series\n",
    "    \"\"\"\n",
    "\n",
    "    hydrological_year = date.year.values.copy()\n",
    "    hydrological_year[date.month >= first_month] += 1\n",
    "\n",
    "    return hydrological_year\n",
    "\n",
    "def run_model_superflexpy(catchments_ids, best_params_dict_model, perm_areas_model):\n",
    "    # Run the iterative function\n",
    "    iterative_immediate_downstream = find_iterative_immediate_downstream(df, catchments_ids)\n",
    "\n",
    "    # Convert results to a DataFrame for display\n",
    "    iterative_downstream_df = pd.DataFrame(iterative_immediate_downstream.items(), \n",
    "                                        columns=['basin_id', 'immediate_downstream_basin'])\n",
    "\n",
    "\n",
    "    # Assuming the DataFrame has columns 'basin_id' and 'downstream_id'\n",
    "    topology_list = {basin: None for basin in catchments_ids}  # Default to None\n",
    "\n",
    "    # Filter DataFrame for relevant basin_ids and update topology\n",
    "    for _, row in iterative_downstream_df.iterrows():\n",
    "        if row['basin_id'] in topology_list:\n",
    "            topology_list[row['basin_id']] = row['immediate_downstream_basin']\n",
    "\n",
    "    # Generate Nodes dynamically and assign them as global variables\n",
    "    catchments = [] # Dictionary to store nodes\n",
    "    \n",
    "    general = Unit(\n",
    "    layers=[\n",
    "        [upper_splitter],\n",
    "        [snow, upper_transparent],\n",
    "        [upper_junction],\n",
    "        [unsaturated],\n",
    "        [lower_splitter],\n",
    "        [slow, lag_fun],\n",
    "        [lower_transparent, fast],\n",
    "        [lower_junction],\n",
    "    ],\n",
    "    id='general')\n",
    "\n",
    "    for cat_id in catchments_ids:\n",
    "        node = Node(\n",
    "            units=[general],  # Use unit from dictionary or default\n",
    "            weights=[1.0],\n",
    "            area=areas.get(cat_id),  # Use predefined area or default\n",
    "            id=cat_id\n",
    "        )\n",
    "        catchments.append(node)  # Store in the list\n",
    "\n",
    "        # Assign the node as a global variable\n",
    "        globals()[cat_id] = node\n",
    "\n",
    "\n",
    "    # Ensure topology only includes nodes that exist in `catchments_ids`\n",
    "    topology = {\n",
    "        cat_id: upstream if upstream in catchments_ids else None\n",
    "        for cat_id, upstream in topology_list.items() if cat_id in catchments_ids\n",
    "    }\n",
    "\n",
    "    # Create the Network\n",
    "    model = Network(\n",
    "        nodes=catchments,  # Pass list of Node objects\n",
    "        topology=topology  \n",
    "    )\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    # Set inputs for each node using the manually defined dictionary\n",
    "    for cat in catchments:\n",
    "        cat.set_input(inputs[cat.id])  # Correct way to set inputs\n",
    "\n",
    "    model.set_timestep(1.0)\n",
    "    model.set_parameters(best_params_dict_model)\n",
    "\n",
    "    output = model.get_output()\n",
    "\n",
    "    return output\n",
    "\n",
    "def is_valid_key(k):\n",
    "    # Exclude keys that end in Group_X_2\n",
    "    return not re.search(r'Group_\\d+_2$', k)\n",
    "\n",
    "def is_valid_key_2(k):\n",
    "    # Include only keys that end in Group_X_2\n",
    "    return re.search(r'Group_\\d+_2$', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a700755",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb17065",
   "metadata": {},
   "source": [
    "### Space-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d65c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:38<00:00,  5.53s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys = [k for k in all_param_dicts if \"nogeo\" in k and is_valid_key(k)]\n",
    "\n",
    "output_regional_dict = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c33a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:35<00:00,  5.09s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys_2 = [k for k in all_param_dicts if \"nogeo\" in k and is_valid_key_2(k)]\n",
    "\n",
    "output_regional_dict_8801 = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict_8801[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12a35136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the concatenated data for the complete series analysis\n",
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "observations1 = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "quality_masks1 = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "\n",
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "observations2 = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "quality_masks2 = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "\n",
    "observations_cal = {}\n",
    "\n",
    "for key in observations1.keys():\n",
    "    arr1 = np.atleast_1d(observations1[key])\n",
    "    arr2 = np.atleast_1d(observations2.get(key, np.array([])))\n",
    "\n",
    "    # Always concatenate arrays, even if they contain NaNs or are empty\n",
    "    #observations_cal[key] = np.concatenate([arr1, arr2])\n",
    "    # Remove the first 365 days from the second dataset\n",
    "    arr2_trimmed = arr2[365:] if arr2.size > 365 else np.array([])\n",
    "\n",
    "    observations_cal[key] = np.concatenate([arr1, arr2_trimmed])\n",
    "\n",
    "quality_masks_cal = {}\n",
    "\n",
    "for key in quality_masks1.keys():\n",
    "    arr1 = np.atleast_1d(quality_masks1[key])\n",
    "    arr2 = np.atleast_1d(quality_masks2.get(key, np.array([])))\n",
    "\n",
    "    # Always concatenate arrays, even if they contain NaNs or are empty\n",
    "    #quality_masks_cal[key] = np.concatenate([arr1, arr2])\n",
    "    # Remove the first 365 days from the second dataset\n",
    "    arr2_trimmed = arr2[365:] if arr2.size > 365 else np.array([])\n",
    "\n",
    "    quality_masks_cal[key] = np.concatenate([arr1, arr2_trimmed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33f96314",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_regional_dict_cal = {}\n",
    "\n",
    "for param_key in output_regional_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_regional_dict_8801:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_regional_dict[param_key]:\n",
    "            if gauge_id in output_regional_dict_8801[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_regional_dict_8801[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_regional_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_8801, series_recent])\n",
    "                \n",
    "                # Remove first 365 days from recent series\n",
    "                series_recent_trimmed = series_recent[365:] if series_recent.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_8801, series_recent_trimmed])\n",
    "\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_regional_dict_cal[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21da17",
   "metadata": {},
   "source": [
    "### Space-time validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4be11a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:35<00:00,  5.12s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_1988_2001'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys = [k for k in all_param_dicts if \"nogeo\" in k and is_valid_key(k)]\n",
    "\n",
    "output_regional_val_dict = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_val_dict[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14e47dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:37<00:00,  5.32s/it]\n"
     ]
    }
   ],
   "source": [
    "path_inputs = '../data/models/input/subset_2001_2015'\n",
    "\n",
    "inputs = np.load(path_inputs+'//inputs.npy', allow_pickle=True).item()\n",
    "observations = np.load(path_inputs+'//observations.npy', allow_pickle=True).item()\n",
    "areas = np.load(path_inputs+'//areas.npy', allow_pickle=True).item()\n",
    "perm_areas = np.load(path_inputs+'//perm_areas.npy', allow_pickle=True).item()\n",
    "perm_areascontinental = np.load(path_inputs+'//perm_areascontinental.npy', allow_pickle=True).item()\n",
    "perm_areasglobal = np.load(path_inputs+'//perm_areasglobal.npy', allow_pickle=True).item()\n",
    "quality_masks = np.load(path_inputs+'//quality_masks.npy', allow_pickle=True).item()\n",
    "rootdepth_mean = np.load(path_inputs+'//rootdepth_mean.npy', allow_pickle=True).item()\n",
    "waterdeficit_mean= np.load(path_inputs+'//waterdeficit_mean.npy', allow_pickle=True).item()\n",
    "\n",
    "# Filter keys\n",
    "regional_keys_2 = [k for k in all_param_dicts if \"nogeo\" in k and is_valid_key_2(k)]\n",
    "\n",
    "output_regional_dict_0115 = {}\n",
    "\n",
    "for key in tqdm.tqdm(regional_keys_2):\n",
    "    output = run_model_superflexpy(\n",
    "        catchments_ids=catchments_ids,\n",
    "        best_params_dict_model=all_param_dicts[key],\n",
    "        perm_areas_model=perm_areas\n",
    "    )\n",
    "    output_regional_dict_0115[key] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8705d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_regional_dict_val = {}\n",
    "\n",
    "for param_key in output_regional_val_dict:\n",
    "    param_key_8801 = param_key + \"_2\"\n",
    "\n",
    "    if param_key_8801 in output_regional_dict_0115:\n",
    "        merged_outputs = {}\n",
    "\n",
    "        for gauge_id in output_regional_val_dict[param_key]:\n",
    "            if gauge_id in output_regional_dict_0115[param_key_8801]:\n",
    "                # Ensure both arrays are flat (1D)\n",
    "                series_8801 = np.ravel(output_regional_dict_0115[param_key_8801][gauge_id])\n",
    "                series_recent = np.ravel(output_regional_val_dict[param_key][gauge_id])\n",
    "                #concatenated = np.concatenate([series_recent, series_8801])\n",
    "\n",
    "                # Remove first 365 days from recent series\n",
    "                series_8801_trimmed = series_8801[365:] if series_8801.size > 365 else np.array([])\n",
    "                concatenated = np.concatenate([series_recent, series_8801_trimmed])\n",
    "\n",
    "                merged_outputs[gauge_id] = [concatenated]\n",
    "\n",
    "        output_regional_dict_val[param_key] = merged_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e011c9f",
   "metadata": {},
   "source": [
    "## Save the time-series in netcdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fe251b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_suffixes = [\"Group_1\", \"Group_2\", \"Group_3\", \"Group_4\", \"Group_5\", \"Group_6\", \"Group_7\"]\n",
    "\n",
    "gauge_ids = list(observations_cal.keys())  # Your observations dict should be preloaded\n",
    "time_index = pd.date_range(start=\"1988-10-01\", end=\"2015-09-30\", freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "648db6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets\n",
    "datasets = {}\n",
    "\n",
    "for suffix in group_suffixes:\n",
    "    reg_key = f\"moselle_best_params_nogeo_{suffix}\"\n",
    "\n",
    "    reg_data = []\n",
    "    cont_data = []\n",
    "    glob_data = []\n",
    "    group_gauge_ids = []\n",
    "\n",
    "    for gauge in gauge_ids:\n",
    "        if gauge in output_regional_dict_val[reg_key]:\n",
    "\n",
    "            reg_data.append(output_regional_dict_val[reg_key][gauge][0])\n",
    "            group_gauge_ids.append(gauge)\n",
    "\n",
    "    if group_gauge_ids:\n",
    "        ds = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"nogeology\": ([\"gauge_id\", \"date\"], reg_data),\n",
    "            },\n",
    "            coords={\n",
    "                \"gauge_id\": group_gauge_ids,\n",
    "                \"date\": time_index\n",
    "            }\n",
    "        )\n",
    "        datasets[suffix] = ds\n",
    "\n",
    "# Save each group to a separate NetCDF file using scipy (no need for netCDF4)\n",
    "for suffix, ds in datasets.items():\n",
    "    ds.to_netcdf(rf\"../results/sim/space-time/notconcatenated/simu_nogeo_{suffix}.nc\", engine=\"scipy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8cd466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets\n",
    "datasets = {}\n",
    "\n",
    "for suffix in group_suffixes:\n",
    "    reg_key = f\"moselle_best_params_nogeo_{suffix}\"\n",
    "\n",
    "    reg_data = []\n",
    "    cont_data = []\n",
    "    glob_data = []\n",
    "    group_gauge_ids = []\n",
    "\n",
    "    for gauge in gauge_ids:\n",
    "        if gauge in output_regional_dict_cal[reg_key]:\n",
    "\n",
    "            reg_data.append(output_regional_dict_cal[reg_key][gauge][0])\n",
    "            group_gauge_ids.append(gauge)\n",
    "\n",
    "    if group_gauge_ids:\n",
    "        ds = xr.Dataset(\n",
    "            data_vars={\n",
    "                \"nogeology\": ([\"gauge_id\", \"date\"], reg_data),\n",
    "            },\n",
    "            coords={\n",
    "                \"gauge_id\": group_gauge_ids,\n",
    "                \"date\": time_index\n",
    "            }\n",
    "        )\n",
    "        datasets[suffix] = ds\n",
    "\n",
    "# Save each group to a separate NetCDF file using scipy (no need for netCDF4)\n",
    "for suffix, ds in datasets.items():\n",
    "    ds.to_netcdf(rf\"../results/sim/space/notconcatenated/simu_nogeo_{suffix}.nc\", engine=\"scipy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ac82e",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
